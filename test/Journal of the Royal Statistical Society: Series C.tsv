History matching of a complex epidemiological model of human immunodeficiency virus transmission by using variance emulation	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12198	AUG 2017	39	Complex stochastic models are commonplace in epidemiology, but their utility depends on their calibration to empirical data. History matching is a (pre)calibration method that has been applied successfully to complex deterministic models. In this work, we adapt history matching to stochastic models, by emulating the variance in the model outputs, and therefore accounting for its dependence on the model's input values. The method proposed is applied to a real complex epidemiological model of human immunodeficiency virus in Uganda with 22 inputs and 18 outputs, and is found to increase the efficiency of history matching, requiring 70% of the time and 43% fewer simulator evaluations compared with a previous variant of the method. The insight gained into the structure of the human immunodeficiency virus model, and the constraints placed on it, are then discussed.	Calibration,Gaussian processes,Individual-based models,Inverse problems,Stochastic simulators	Andrianakis, I.@London Sch Hyg & Trop Med, London, England::Vernon, I.@Univ Durham, Durham, England::McCreesh, N.@London Sch Hyg & Trop Med, London, England::McKinley, T. J.@Univ Exeter, Exeter, Devon, England::Oakley, J. E.@Univ Sheffield, Sheffield, S Yorkshire, England::Nsubuga, R. N.@Med Res Council Uganda, Kampala, Uganda::Goldstein, M.@Univ Durham, Durham, England::White, R. G.@Univ Durham, Durham, England
Movers and stayers in the farming sector: accounting for unobserved heterogeneity in structural change	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12196	AUG 2017	34	The paper investigates whether accounting for unobserved heterogeneity in farms' size transition processes improves the representation of structural change in agriculture. Considering a mixture of two types of farm, the mover-stayer model is applied for the first time in an agricultural economics context. The maximum likelihood method and the expectation-maximization algorithm are used to estimate the model's parameters. An empirical application to a panel of French farms from 2000 to 2013 shows that the mover-stayer model outperforms the homogeneous Markov chain model in recovering the transition process and predicting the future distribution of farm sizes.	EM algorithm,Farms,Markov chain,Mover-stayer model,Structural change,Unobserved heterogeneity	Saint-Cyr, Legrand D. F.@Agrocampus Ouest, Rennes, France::Piet, Laurent@Agrocampus Ouest, Rennes, France
Bayesian causality test for integer-valued time series models with applications to climate and crime data	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12200	AUG 2017	36	We investigate the causal relationship between climate and criminal behaviour. Considering the characteristics of integer-valued time series of criminal incidents, we propose a modified Granger causality test based on the generalized auto-regressive conditional heteroscedasticity type of integer-valued time series models to analyse the relationship between the number of crimes and the temperature as an environmental factor. More precisely, we employ the Poisson, negative binomial and log-linear Poisson integer-valued generalized auto-regressive conditional heteroscedasticity models and particularly adopt a Bayesian method for our analysis. The Bayes factors and posterior probability of the null hypothesis help to determine the causality between the variables considered. Moreover, employing an adaptive Markov chain Monte Carlo sampling scheme, we estimate model parameters and initial values. As an illustration, we evaluate our test through a simulation study and, to examine whether or not temperature affects crime activities, we apply our method to data sets categorized as sexual offences, drug offences, theft of motor vehicles, and domestic-violence-related assault in Ballina, New South Wales, Australia. The result reveals that more sexual offences, drug offences and domestic-violence-related assaults occur during the summer than in other seasons of the year. This evidence strongly advocates a causal relationship between crime and temperature.	Concurrent linear relationship,Granger causality test,Markov chain Monte Carlo method,Negative binomial integer-valued generalized auto-regressive conditional heteroscedasticity model,Poisson integer-valued generalized auto-regressive conditional heteroscedasticity model,Posterior odds ratio	Chen, Cathy W. S.@Feng Chia Univ, Taichung, Taiwan::Lee, Sangyeol@Seoul Natl Univ, Seoul, South Korea
A second-order semiparametric method for survival analysis, with application to an acquired immune deficiency syndrome clinical trial study	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12189	AUG 2017	14	Motivated by the recent acquired immune deficiency syndrome clinical trial study A5175, we propose a semiparametric framework to describe time-to-event data, where only the dependence of the mean and variance of the time on the covariates are specified through a restricted moment model. We use a second-order semiparametric efficient score combined with a non-parametric imputation device for estimation. Compared with an imputed weighted least squares method, the approach proposed improves the efficiency of the parameter estimation whenever the third moment of the error distribution is non-zero. We compare the method with a parametric survival regression method in the A5175 study data analysis. In the data analysis, the method proposed shows a better fit to the data with smaller mean-squared residuals. In summary, this work provides a semiparametric framework in modelling and estimation of survival data. The framework has wide applications in data analysis.	CD4 cell counts,Censoring,Efficiency,Imputation,Kernel,Non-parametric methods,Restricted moments,Safety end points,Two-stage analysis,Toxicity	Jiang, Fei@Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China::Ma, Yanyuan@Penn State Univ, State Coll, PA USA::Lee, J. Jack@Univ Texas MD Anderson Canc Ctr, Houston, TX 77030 USA
Implementing propensity score matching with network data: the effect of the General Agreement on Tariffs and Trade on bilateral trade	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12173	APR 2017	35	Motivated by the evaluation of the causal effect of the General Agreement on Tariffs and Trade on bilateral international trade flows, we investigate the role of network structure in propensity score matching under the assumption of strong ignorability. We study the sensitivity of causal inference with respect to the presence of characteristics of the network in the set of confounders conditionally on which strong ignorability is assumed to hold. We find that estimates of the average causal effect are highly sensitive to the node level network statistics in the set of confounders. Therefore, we argue that estimates may suffer from omitted variable bias when the network information is ignored, at least in our application.	Centrality,Clustering,General Agreement on Tariffs and Trade,Matching,Networks,Trade,Unconfoundedness	Arpino, Bruno@Univ Pompeu Fabra, Barcelona, Spain::De Benedictis, Luca@Univ Macerata, Macerata, Italy::Mattei, Alessandra@Univ Florence, Florence, Italy
Simultaneous inference for multilevel linear mixed modelswith an application to a large-scale school meal study	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12161	FEB 2017	33	In large multilevel studies effects of interest are often evaluated for a number of more or less related outcomes. For instance, the present work was motivated by the multiplicity issues that arose in the analysis of a cluster-randomized, crossover intervention study evaluating the health benefits of a school meal programme. We propose a novel and versatile framework for simultaneous inference on parameters estimated from linear mixed models that were fitted separately for several outcomes from the same study, but did not necessarily contain the same fixed or random effects. By combining asymptotic representations of parameter estimates from separate model fits we could derive the joint asymptotic normal distribution for all parameter estimates of interest for all outcomes considered. This result enabled the construction of simultaneous confidence intervals and calculation of adjusted p-values. For sample sizes of practical relevance we studied simultaneous coverage through simulation, which showed that the approach achieved acceptable coverage probabilities even for small sample sizes (10 clusters) and for 2-16 outcomes. The approach also compared favourably with a joint modelling approach. We also analysed data with 17 outcomes from the motivating study, resulting in adjusted p-values that were appreciably less conservative than Bonferroni adjustment.	Asymptotic normality,Decorrelation,Familywise error rate,Generalized least squares	Ritz, Christian@Univ Copenhagen, Copenhagen, Denmark::Laursen, Rikke Pilmann@Univ Copenhagen, Copenhagen, Denmark::Damsgaard, Camilla Trab@Univ Copenhagen, Copenhagen, Denmark
An adaptive spatiotemporal smoothing model for estimating trends and step changes in disease risk	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12155	JAN 2017	32	Statistical models used to estimate the spatiotemporal pattern in disease risk from areal unit data represent the risk surface for each time period with known covariates and a set of spatially smooth random effects. The latter act as a proxy for unmeasured spatial confounding, whose spatial structure is often characterized by a spatially smooth evolution between some pairs of adjacent areal units whereas other pairs exhibit large step changes. This spatial heterogeneity is not consistent with existing global smoothing models, in which partial correlation exists between all pairs of adjacent spatial random effects. Therefore we propose a novel space-time disease model with an adaptive spatial smoothing specification that can identify step changes. The model is motivated by a new study of respiratory and circulatory disease risk across the set of local authorities in England and is rigorously tested by simulation to assess its efficacy. Results from the England study show that the two diseases have similar spatial patterns in risk and exhibit some common step changes in the unmeasured component of risk between neighbouring local authorities.	Adaptive smoothing,Gaussian Markov random fields,Spatiotemporal disease mapping,Step change detection	Rushworth, Alastair@Univ Strathclyde, Glasgow, Lanark, Scotland::Lee, Duncan@Univ Glasgow, Glasgow G12 8QQ, Lanark, Scotland::Sarran, Christophe@UK Met Off, Exeter, Devon, England
A dose-schedule finding design for phase I-II clinical trials	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12113	FEB 2016	20	Dose finding methods aiming at identifying an optimal dose of a treatment with a given schedule may be at a risk of misidentifying the best treatment for patients. We propose a phase I-II clinical trial design to find the optimal dose-schedule combination. We define schedule as the method and timing of administration of a given total dose in a treatment cycle. We propose a Bayesian dynamic model for the joint effects of dose and schedule. The model proposed allows us to borrow strength across dose-schedule combinations without making overly restrictive assumptions on the ordering pattern of the schedule effects. We develop a dose-schedule finding algorithm to allocate patients sequentially to a desirable dose-schedule combination, and to select an optimal combination at the end of the trial. We apply the proposed design to a phase I-II clinical trial of a -secretase inhibitor in patients with refractory metastatic or locally advanced solid tumours, and we examine the operating characteristics of the design through simulations.	Bayesian dynamic model,Dose-schedule combination,Efficacy,Probit model,Schedule-response relationship,Toxicity	Guo, Beibei@Louisiana State Univ, Baton Rouge, LA 70803 USA::Li, Yisheng@Univ Texas MD Anderson Canc Ctr, Houston, TX 77030 USA::Yuan, Ying@Univ Texas MD Anderson Canc Ctr, Houston, TX 77030 USA
A Bayesian approach to estimate changes in condom use from limited human immunodeficiency virus prevalence data	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12116	FEB 2016	37	Evaluation of large-scale intervention programmes against human immunodeficiency virus (HIV) is becoming increasingly important, but impact estimates frequently hinge on knowledge of changes in behaviour such as the frequency of condom use over time, or other self-reported behaviour changes, for which we generally have limited or potentially biased data. We employ a Bayesian inference methodology that incorporates an HIV transmission dynamics model to estimate condom use time trends from HIV prevalence data. Estimation is implemented via particle Markov chain Monte Carlo methods, applied for the first time in this context. The preliminary choice of the formulation for the time varying parameter reflecting the proportion of condom use is critical in the context studied, because of the very limited amount of condom use and HIV data available. We consider various novel formulations to explore the trajectory of condom use over time, based on diffusion-driven trajectories and smooth sigmoid curves. Numerical simulations indicate that informative results can be obtained regarding the amplitude of the increase in condom use during an intervention, with good levels of sensitivity and specificity performance in effectively detecting changes. The application of this method to a real life problem demonstrates how it can help in evaluating HIV interventions based on a small number of prevalence estimates, and it opens the way to similar applications in different contexts.	Bayesian inference,Condom use,Epidemic modelling,Human immunodeficiency virus infections,Particle Markov chain Monte Carlo methods,Time varying parameter	Dureau, J.@Univ London London Sch Econ & Polit Sci, London WC2A 2AE, England::Kalogeropoulos, K.@Univ London London Sch Econ & Polit Sci, London WC2A 2AE, England::Vickerman, P.@London Sch Hyg & Trop Med, London, England::Pickles, M.@Univ London Imperial Coll Sci Technol & Med, London SW7 2AZ, England::Boily, M. -C.@Univ London Imperial Coll Sci Technol & Med, London SW7 2AZ, England
Comparing two binary diagnostic tests with repeated measurements	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12122	FEB 2016	18	We compare two binary diagnostic tests when each subject is measured more than once with each test and with a gold standard. We introduce a new model that allows the correlation between two measurements on a single subject by the same test to be different from the correlation between two measurements by different tests. We show that moment estimators of the population parameters for the mean sensitivities and specificities are virtually identical to the maximum likelihood estimates from our random-effects model. We apply the model to data comparing two rapid malaria tests and provide guidance for choosing the number of subjects and repeated measurements.	Diagnostic tests,Gold standard,Moment estimators,Random effects,Repeated testing,Sensitivity and specificity	Steiner, Stefan H.@Univ Waterloo, Waterloo, ON N2L 3G1, Canada::Danila, Oana@F Hoffmann La Roche & Co Ltd, CH-4002 Basel, Switzerland::Cotton, Cecilia A.@Univ Waterloo, Waterloo, ON N2L 3G1, Canada::Severn, Daniel@Univ Waterloo, Waterloo, ON N2L 3G1, Canada::Mackay, R. Jock@Univ Waterloo, Waterloo, ON N2L 3G1, Canada
Estimating controlled direct effects of restrictive feeding practices in the 'Early dieting in girls' study	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12109	JAN 2016	40	We examine the causal effect of parental restrictive feeding practices on children's weight status. An important mediator is children's self-regulation status. Recent approaches interpret mediation effects on the basis of the potential outcomes framework. Inverse probability weighting based on propensity scores are used to adjust for confounding and to reduce the dimensionality of confounders simultaneously. We show that combining machine learning algorithms and logistic regression to estimate the propensity scores can be more accurate and efficient in estimating the controlled direct effects than using logistic regression alone. A data application shows that the causal effect of mother's restrictive feeding differs according to whether the daughter eats in the absence of hunger.	Causal inference,Controlled direct effects,Generalized boosted model,Mediator,Model combining,Random forests	Zhu, Yeying@Univ Waterloo, Waterloo, ON N2L 3G1, Canada::Ghosh, Debashis@Colorado Sch Publ Hlth, Aurora, CO USA::Coffman, Donna L.@Penn State Univ, University Pk, PA 16802 USA::Savage, Jennifer S.@Penn State Univ, University Pk, PA 16802 USA
Probabilistic principal component analysis to identify profiles of physical activity behaviours in the presence of non-ignorable missing data	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12105	JAN 2016	64	The paper is motivated by an accelerometer-based study of physical activity (PA) behaviours in a large cohort of UK school-aged children. Advances in research on PA are accompanied by a growing number of results that are contributing to form a complex picture of PA behaviours in children. One source of such complexity is intimately related to the multiplicity of dimensions associated with PA. Currently a comprehensive individual accelerometer summary can include a large number of outcomes and this clearly poses challenges for the analysis. We explore the application of principal component analysis to accelerometer measurements that are aggregated daily over several days of the week and are affected by missingness. The probabilistic approach to principal component analysis with latent scores is extended to include non-ignorable missing data. The extended likelihood is maximized through a Monte Carlo EM algorithm via adaptive rejection Metropolis sampling. Our findings suggest that physical activity and inactivity are two dimensions over which children aggregate into distinct behavioural profiles, characterized by gender and season but not by anthropometric factors.	Body mass index,Latent scores,Millennium Cohort Study,Obesity,Sedentary behaviour	Geraci, Marco@Univ S Carolina, Columbia, SC 29208 USA::Farcomeni, Alessio@Univ Roma La Sapienza, Rome, Italy
Partially latent class models for case-control studies of childhood pneumonia aetiology	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12101	JAN 2016	26	In population studies on the aetiology of disease, one goal is the estimation of the fraction of cases that are attributable to each of several causes. For example, pneumonia is a clinical diagnosis of lung infection that may be caused by viral, bacterial, fungal or other pathogens. The study of pneumonia aetiology is challenging because directly sampling from the lung to identify the aetiologic pathogen is not standard clinical practice in most settings. Instead, measurements from multiple peripheral specimens are made. The paper introduces the statistical methodology designed for estimating the population aetiology distribution and the individual aetiology probabilities in the Pneumonia Etiology Research for Child Health study of 9500 children for seven sites around the world. We formulate the scientific problem in statistical terms as estimating the mixing weights and latent class indicators under a partially latent class model (PLCM) that combines heterogeneous measurements with different error rates obtained from a case-control study. We introduce the PLCM as an extension of the latent class model. We also introduce graphical displays of the population data and inferred latent class frequencies. The methods are tested with simulated data, and then applied to Pneumonia Etiology Research for Child Health data. The paper closes with a brief description of extensions of the PLCM to the regression setting and to the case where conditional independence between the measures is relaxed.	Aetiology,Bayesian method,Case-control,Latent class,Measurement error,Pneumonia	Wu, Zhenke@Johns Hopkins Univ, Baltimore, MD 21205 USA::Deloria-Knoll, Maria@Johns Hopkins Univ, Baltimore, MD 21205 USA::Hammitt, Laura L.@Johns Hopkins Univ, Baltimore, MD 21205 USA::Zeger, Scott L.@Johns Hopkins Univ, Baltimore, MD 21205 USA
Predicting health programme participation: a gravity-based, hierarchical modelling approach	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12111	JAN 2016	46	Statistical analyses of health programme participation seek to address a number of objectives that are compatible with the evaluation of demand for current resources. In this spirit, a spatial hierarchical model is developed for disentangling patterns in participation at the small area level, as a function of population-based demand and additional variation. For the former, a constrained gravity model is proposed to quantify factors associated with spatial choice and to account for competition effects, for programmes delivered by multiple clinics. The implications of gravity model misspecification within a mixed effects framework are also explored. The model proposed is applied to participation data from a no-fee mammography programme in Brisbane, Australia. Attention is paid to the interpretation of various model outputs and their relevance for public health policy.	Bayesian methods,Gravity model,Health services research,Log-linear model,Markov chain Monte Carlo methods,Random effects	
Modelling the type and timing of consecutive events: application to predicting preterm birth in repeated pregnancies	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12100	NOV 2015	13	Predicting the occurrence and timing of adverse pregnancy events such as preterm birth is an important analytical challenge in obstetrical practice. Developing statistical approaches that can be used to assess the risk and timing of these adverse events will provide clinicians with tools for individualized risk assessment that account for a woman's prior pregnancy history. Often adverse pregnancy outcomes are subject to competing events; for example, interest may focus on the occurrence of pre-eclampsia-related preterm birth, where preterm birth for other reasons may serve as a competing event. We propose modelling the type and timing of adverse outcomes in repeated pregnancies. We formulate a joint model, where types of adverse outcomes across repeated pregnancies are modelled by using a polychotomous logistic regression model with random effects, and gestational ages at delivery are modelled conditionally on the types of adverse outcome. The correlation between gestational ages conditional on the adverse pregnancies is modelled by the semiparametric normal copula function. We present a two-stage estimation method and develop the asymptotic theory for the estimators proposed. The model and estimation procedure proposed are applied to the National Institute of Child Health and Human Development consecutive pregnancies study data and evaluated by simulations.	Adverse pregnancy outcome,Normal copula,Polychotomous random-effects logistic model,Pre-eclampsia,Preterm birth,Repeated pregnancies	Shih, Joanna H.@NCI, Bethesda, MD 20892 USA::Albert, Paul S.@Eunice Kennedy Shriver Natl Inst Child Hlth & Hum, Rockville, MD USA::Mendola, Pauline@Eunice Kennedy Shriver Natl Inst Child Hlth & Hum, Rockville, MD USA::Grantz, Katherine L.@Eunice Kennedy Shriver Natl Inst Child Hlth & Hum, Rockville, MD USA
Bayesian structured additive distributional regression for multivariate responses	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12090	AUG 2015	49	We propose a unified Bayesian approach for multivariate structured additive distributional regression analysis comprising a huge class of continuous, discrete and latent multivariate response distributions, where each parameter of these potentially complex distributions is modelled by a structured additive predictor. The latter is an additive composition of different types of covariate effects, e.g. non-linear effects of continuous covariates, random effects, spatial effects or interaction effects. Inference is realized by a generic, computationally efficient Markov chain Monte Carlo algorithm based on iteratively weighted least squares approximations and with multivariate Gaussian priors to enforce specific properties of functional effects. Applications to illustrate our approach include a joint model of risk factors for chronic and acute childhood undernutrition in India and ecological regressions studying the drivers of election results in Germany.	Correlated responses,Dirichlet regression,Iteratively weighted least squares proposal,Markov chain Monte Carlo simulation,Penalized splines,Seemingly unrelated regression	Klein, Nadja@Univ Gottingen, D-37073 Gottingen, Germany::Kneib, Thomas@Univ Gottingen, D-37073 Gottingen, Germany::Klasen, Stephan@Univ Gottingen, D-37073 Gottingen, Germany::Lang, Stefan@Univ Innsbruck, A-6020 Innsbruck, Austria
A marginal cure rate proportional hazards model for spatial survival data	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12098	AUG 2015	37	Dental studies often produce spatially referenced multivariate time-to-event data, such as the time until tooth loss due to periodontal disease. These data are used to identify risk factors that are associated with tooth loss, and to predict outcomes for an individual patient. The rate of spatial referencing can vary with various tooth locations. In addition, these event time data are heavily censored, mostly because a certain proportion of teeth in the population are not expected to experience failure and can be considered cured'. We assume a proportional hazards model with a surviving fraction to model these clustered correlated data and account for dependence between nearby teeth by using spatial frailties which are modelled as linear combinations of positive stable random effects. This model permits predictions (conditioned on spatial frailties) that account for the survival status of nearby teeth and simultaneously preserves the proportional hazards relationship marginally over the random effects for the susceptible teeth, allowing for interpretable estimates of the effects of risk factors on tooth loss. We explore the potential of this model via simulation studies and application to a real data set obtained from a private periodontal practice, and we illustrate its advantages over other competing models to identify important risk factors for tooth loss and to predict the remaining lifespan of a patient's teeth.	Bayesian hierarchical modelling,Cure rate,Dental data,Extreme value analysis,Frailty,Positive stable,Tooth loss	Schnell, Patrick@Univ Minnesota, Minneapolis, MN USA::Bandyopadhyay, Dipankar@Univ Minnesota, Minneapolis, MN USA::Reich, Brian J.@N Carolina State Univ, Raleigh, NC 27695 USA::Nunn, Martha@Creighton Univ, Omaha, NE 68178 USA
Optimal retesting configurations for hierarchical group testing	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12097	AUG 2015	18	Hierarchical group testing is widely used to test individuals for diseases. This testing procedure works by first amalgamating individual specimens into groups for testing. Groups testing negatively have their members declared negative. Groups testing positively are subsequently divided into smaller subgroups and are then retested to search for positive individuals. We propose a new class of informative retesting procedures for hierarchical group testing that acknowledges heterogeneity among individuals. These procedures identify the optimal number of groups and their sizes at each testing stage to minimize the expected number of tests. We apply our proposals in two settings: human immunodeficiency virus testing programmes that currently use three-stage hierarchical testing and chlamydia and gonorrhoea screening practices that currently use individual testing. For both applications, we show that substantial savings can be realized by our new procedures.	Classification,Human immunodeficiency virus,Infertility prevention project,Informative retesting,Pooled testing,Retesting	Black, Michael S.@Univ Wisconsin Platteville, Platteville, WI USA::Bilder, Christopher R.@Univ Nebraska, Lincoln, NE 68583 USA::Tebbs, Joshua M.@Univ S Carolina, Columbia, SC 29208 USA
Bayesian optimal interval designs for phase I clinical trials	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12089	APR 2015	22	In phase I trials, effectively treating patients and minimizing the chance of exposing them to subtherapeutic and overly toxic doses are clinicians' top priority. Motived by this practical consideration, we propose Bayesian optimal interval (BOIN) designs to find the maximum tolerated dose and to minimize the probability of inappropriate dose assignments for patients. We show, both theoretically and numerically, that the BOIN design not only has superior finite and large sample properties but also can be easily implemented in a simple way similar to the traditional 3+3' design. Compared with the well-known continual reassessment method, the BOIN design yields comparable average performance to select the maximum tolerated dose but has a substantially lower risk of assigning patients to subtherapeutic and overly toxic doses. We apply the BOIN design to two cancer clinical trials.	Bayesian adaptive design,Decision error,Dose finding,Maximum tolerated dose	Liu, Suyu@Univ Texas MD Anderson Canc Ctr, Houston, TX 77030 USA::Yuan, Ying@Univ Texas MD Anderson Canc Ctr, Houston, TX 77030 USA
Group sequential method for observational data by using generalized estimating equations: application to Vaccine Safety Datalink	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12076	FEB 2015	18	Post-market medical product surveillance is important for detecting rare adverse events that are not identified during preapproval. The goal of surveillance is to assess over time for elevated rates of adverse events for new medical products. These studies utilize administrative databases from multiple large health plans. We propose a group sequential method using a permutation approach with generalized estimating equations to account for confounding. A simulation study is conducted to evaluate the performance of the group sequential generalized estimating equation method compared with two other approaches. The methods are then applied to a vaccine safety application from the Vaccine Safety Datalink.	Drug safety,Generalized estimating equations,Group sequential method for observational studies,Post-marketing surveillance,Vaccine safety	Cook, Andrea J.@Grp Hlth Res Inst, Seattle, WA 98101 USA::Wellman, Robert D.@Grp Hlth Res Inst, Seattle, WA 98101 USA::Nelson, Jennifer C.@Grp Hlth Res Inst, Seattle, WA 98101 USA::Jackson, Lisa A.@Grp Hlth Res Inst, Seattle, WA 98101 USA::Tiwari, Ram C.@US FDA, Silver Spring, MD USA
Predicting time to threshold for initiating antiretroviral treatment to evaluate cost of treatment as prevention of human immunodeficiency virus	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12080	FEB 2015	32	The goal of the paper is to predict the additional amount of antiretroviral treatment that would be required to implement a policy of treating all human immunodeficiency virus (HIV) infected people at the time of detection of infection rather than at the time that their CD4 T-lymphocyte counts are observed to be below a thresholdthe current standard of care. We describe a sampling-based inverse prediction method for predicting time from HIV infection to attainment of the CD4 cell threshold and apply it to a set of treatment naive HIV-infected subjects in a village in Botswana who participated in a household survey that collected cross-sectional CD4 cell counts. The inferential target of interest is the population level mean time to reaching the CD4 cell-based treatment threshold in this group of subjects. To address the challenges arising from the fact that these subjects' dates of HIV infection are unknown, we make use of data from an auxiliary cohort study of subjects enrolled shortly after HIV infection in which CD4 cell counts were measured over time. We use a multiple-imputation framework to combine across the different sources of data, and we discuss how the methods compensate for the length-biased sampling that is inherent in cross-sectional screening procedures, such as household surveys. We comment on how the results bear on analyses of costs of implementation of treatment-for-prevention use of antiretroviral drugs in HIV prevention interventions.	CD4 cell threshold,Human immunodeficiency virus,Inverse prediction,Longitudinal studies,Multiple imputation	Lynch, Miranda L.@Univ Connecticut, Ctr Hlth, Farmington, CT 06030 USA::DeGruttola, Victor@Harvard Univ, Sch Publ Hlth, Boston, MA 02115 USA
Inferences on relative failure rates in stratified mark-specific proportional hazards models with missing marks, with application to human immunodeficiency virus vaccine efficacy trials	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12067	JAN 2015	25	The paper develops hypothesis testing procedures for the stratified mark-specific proportional hazards model in the presence of missing marks. The motivating application is preventive human immunodeficiency virus (HIV) vaccine efficacy trials, where the mark is the genetic distance of an infecting HIV sequence to an HIV sequence represented inside the vaccine. The test statistics are constructed on the basis of two-stage efficient estimators, which utilize auxiliary predictors of the missing marks. The asymptotic properties and finite sample performances of the testing procedures are investigated, demonstrating double robustness and effectiveness of the predictive auxiliaries to recover efficiency. The methods are applied to the RV144 vaccine trial.	Augmented inverse probability weighting,Auxiliary marks,Competing risks failure time data,Genetic data,Proportional hazards model,Semiparametric model	Gilbert, Peter B.@Univ Washington, Seattle, WA 98195 USA::Sun, Yanqing@Univ N Carolina, Charlotte, NC 28223 USA
Pseudoempirical-likelihood-based method using calibration for longitudinal data with dropout	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12063	JAN 2015	27	In observational studies, interest mainly lies in estimation of the population level relationship between the explanatory variables and dependent variables, and the estimation is often undertaken by using a sample of longitudinal data. In some situations, the longitudinal data sample features biases and loss of estimation efficiency due to non-random dropout. However, inclusion of population level information can increase estimation efficiency. We propose an empirical-likelihood-based method to incorporate population level information in a longitudinal study with dropout. The population level information is incorporated via constraints on functions of the parameters, and non-random dropout bias is corrected by using a weighted generalized estimating equations method. We provide a three-step estimation procedure that makes computation easier. Some commonly used methods are compared in simulation studies, which demonstrate that our proposed method can correct the non-random dropout bias and increase the estimation efficiency, especially for small sample sizes or when the missing proportion is high. In some situations, the improvement in efficiency is substantial. Finally, we apply the method to an Alzheimer's disease study.	Calibration,Dropout,Empirical likelihood,Longitudinal data	Chen, Baojiang@Univ Nebraska Med Ctr, Omaha, NE 68198 USA::Zhou, Xiao-Hua@Univ Washington, Seattle, WA 98195 USA::Chan, Gary@Univ Washington, Seattle, WA 98195 USA
Inferences on lung cancer mortality rates based on reference priors under partial ordering	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12059	NOV 2014	29	We present a novel analysis of a landmark table of dose-response mortality counts from lung cancer in men. The data were originally collected by Doll and Hill. Our inferences are based on Poisson models for which the rates of occurrence are partially ordered according to two covariates. The partial ordering of the mortality rates enforces the well-established knowledge that lung cancer mortality rates are higher for older men and for heavier smokers. The ordered group reference priors that we use in our analyses generalize a class of reference priors that we previously derived for models of count data in which the rates of occurrence in different categories are completely ordered with respect to the values of a single covariate. The reference models for the lung cancer data based on the proposed priors are more flexible than and can be superior, in terms of goodness of fit, to a Bayesian version of several parametric models derived from a mathematical theory of carcinogenesis that have appeared in the literature.	Markov chain Monte Carlo methods,Non-informative prior,Posterior predictive diagnostics,Smoking mortality	Sonksen, Michael D.@Univ New Mexico, Albuquerque, NM 87131 USA::Peruggia, Mario@Ohio State Univ, Columbus, OH 43210 USA
Inference for stable isotope mixing models: a study of the diet of dunlin	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12047	AUG 2014	26	Stable isotope sourcing is used to estimate proportional contributions of sources to a mixture, such as in the analysis of animal diets and plant nutrient use. Statistical methods for inference on the diet proportions by using stable isotopes have focused on the linear mixing model. Existing frequentist methods assume that the diet proportion vector can be uniquely solved for in terms of one or two isotope ratios. We develop large sample methods that apply to an arbitrary number of isotope ratios, assuming that the linear mixing model has a unique solution or is overconstrained. We generalize these methods to allow temporal modelling of the population mean diet, assuming that isotope ratio response data are collected over time. The methodology is motivated by a study of the diet of dunlin, a small migratory seabird.	Animal ecology,Estimating equations,Least squares,Resource utilization	Erhardt, Erik Barry@Univ New Mexico, Albuquerque, NM 87131 USA::Bedrick, Edward J.@Univ New Mexico, Albuquerque, NM 87131 USA
Estimating the burden of pertussis in Mexican adolescents from paired serological data by using a bivariate mixture model	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12051	AUG 2014	20	In recent decades there has been an increase in the reported incidence of clinical pertussis in many countries. Estimation of the true circulation of the bacterium Bordetella pertussis is most reliably made on the basis of studies that measure antibody concentrations against pertussis toxin. Antibody levels decay over time and provide a fading memory of the infection. We develop a discrete bivariate mixture model for paired antibody levels in a cohort of 1002 Mexican adolescents who were followed over the 2008-2009 school year. This model postulates three groups of children based on past pertussis infection; never, prior and new. On the basis of this model we directly estimate incidence and prevalence, and select a diagnostic cut-off for classifying children as recently infected. We also discuss a relatively simple approach that uses only 'discordant' children who test positively on one visit and negatively on the other. The discordant approach provides inferences that are very similar to those of the full model when the data follow the assumed full model. Additionally, the discordant method is much more robust to model misspecification than the full model which has substantial problems with optimization. We estimate the school year incidence of pertussis to be about 3% and the prevalence to be about 8%. A cut-off of 50 was estimated to have about 99.5% specificity and 68% sensitvity.	Conditioning,Cut-off value,Discordance,Empirical Bayes posterior probabilities	Follmann, Dean@NIAID, Bethesda, MD 20892 USA::Qin, Jing); Guerrero, ML (Guerrero, M. Lourdes@Inst Nacl Ciencias Med & Nutr Salvador Zubiran, Mexico City, DF, Mexico::Breugelmans, J. Gabrielle@Agcy Prevent Med, Paris, France::Pedraza, Gustave Rosales@Inst Nacl Ciencias Med & Nutr Salvador Zubiran, Mexico City, DF, Mexico::Gessner, Bradford D.@Agcy Prevent Med, Paris, France::Ruiz-Palacios, Guillermo M.@Inst Nacl Ciencias Med & Nutr Salvador Zubiran, Mexico City, DF, Mexico
Emulating a gravity model to infer the spatiotemporal dynamics of an infectious disease	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12042	APR 2014	51	Probabilistic models for infectious disease dynamics are useful for understanding the mechanism underlying the spread of infection. When the likelihood function for these models is expensive to evaluate, traditional likelihood-based inference may be computationally intractable. Furthermore, traditional inference may lead to poor parameter estimates and the fitted model may not capture important biological characteristics of the observed data. We propose a novel approach for resolving these issues that is inspired by recent work in emulation and calibration for complex computer models. Our motivating example is the gravity time series susceptible-infected-recovered model. Our approach focuses on the characteristics of the process that are of scientific interest. We find a Gaussian process approximation to the gravity model by using key summary statistics obtained from model simulations. We demonstrate via simulated examples that the new approach is computationally expedient, provides accurate parameter inference and results in a good model fit. We apply our method to analyse measles outbreaks in England and Wales in two periods: the prevaccination period from 1944 to 1965 and the vaccination period from 1966 to 1994. On the basis of our results, we can obtain important scientific insights about the transmission of measles. In general, our method is applicable to problems where traditional likelihood-based inference is computationally intractable or produces a poor model fit. It is also an alternative to approximate Bayesian computation when simulations from the model are expensive.	Infectious diseases,Gaussian processes,Expensive likelihood,Computer model emulation,Calibration,Susceptible-infected-recovered model	Jandarov, Roman@Univ Washington, Seattle, WA 98195 USA::Haran, Murali@Penn State Univ, University Pk, PA USA::Bjornstad, Ottar@Penn State Univ, University Pk, PA USA::Grenfell, Bryan@Princeton Univ, Princeton, NJ 08544 USA
Statistical inference and computational efficiency for spatial infectious disease models with plantation data	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12036	APR 2014	16	The paper considers data from an aphid infestation on a sugar cane plantation and illustrates the use of an individual level infectious disease model for making inference on the biological process underlying these data. The data are interval censored, and the practical issues involved with the use of Markov chain Monte Carlo algorithms with models of this sort are explored and developed. As inference for spatial infectious disease models is complex and computationally demanding, emphasis is put on a minimal parsimonious model and speed of code execution. With careful coding we can obtain highly efficient Markov chain Monte Carlo algorithms based on a simple random-walk Metropolis-within-Gibbs routine. An assessment of model fit is provided by comparing the predicted numbers of weekly infections from the data to the trajectories of epidemics simulated from the posterior distributions of model parameters. This assessment shows that the data have periods where the epidemic proceeds more slowly and more quickly than the (temporally homogeneous) model predicts.	Individual level models,Markov chain Monte Carlo methods,Spatial statistics	Brown, Patrick E.@Canc Care Ontario, Toronto, ON M5G 2L7, Canada::Chimard, Florencia@Univ Antilles Guyane, Pointe A Pitre, Guadeloupe::Remorov, Alexander@Univ Toronto, Toronto, ON M5S 1A1, Canada::Rosenthal, Jeffrey S.@Univ Toronto, Toronto, ON M5S 1A1, Canada::Wang, Xin@Univ Toronto, Toronto, ON M5S 1A1, Canada
A generalized quasi-likelihood scoring approach for simultaneously testing the genetic association of multiple traits	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12038	APR 2014	34	In the genetic association analysis of Holstein cattle data, researchers are interested in testing the association between a genetic marker with more than one estimated breeding value phenotype. It is well known that testing each trait individually may lead to problems of controlling the overall type I error rate and simultaneous testing of the association between a marker and multiple traits is desired. The analysis of Holstein cattle data has additional challenges due to complicated relationships between subjects. Furthermore, phenotypic data in many other genetic studies can be quantitative, binary, ordinal, count data or a combination of different types of data. Motivated by these problems, we propose a novel statistical method that allows simultaneous testing of multiple phenotypes and the flexibility to accommodate data from a broad range of study designs. The empirical results indicate that this new method effectively controls the overall type I error rate at the desired level; it is also generally more powerful than testing each trait individually at a given overall type I error rate. The method is applied to the analysis of Holstein cattle data as well as to data from the Collaborative Study on the Genetics of Alcoholism to demonstrate the flexibility of the approach with different phenotypic data types.	Genetic association study,Multiple testing,Quasi-likelihood	Feng, Zeny@Univ Guelph, Guelph, ON N1G 2W1, Canada
Longitudinal analysis of self-reported health status by mixture latent auto-regressive models	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12030	FEB 2014	51	Motivated by an application to a longitudinal data set coming from the Health and Retirement Study about self-reported health status, we propose a model for longitudinal data which is based on a latent process to account for the unobserved heterogeneity between sample units in a dynamic fashion. The latent process is modelled by a mixture of auto-regressive AR(1) processes with different means and correlation coefficients, but with equal variances. We show how to perform maximum likelihood estimation of the proposed model by the joint use of an expectation-maximization algorithm and a Newton-Raphson algorithm, implemented by means of recursions developed in the hidden Markov model literature. We also introduce a simple method to obtain standard errors for the parameter estimates and suggest a strategy to choose the number of mixture components. In the application the response variable is ordinal; however, the approach may also be applied in other settings. Moreover, the application to the self-reported health status data set allows us to show that the model proposed is more flexible than other models for longitudinal data based on a continuous latent process. The model also achieves a goodness of fit that is similar to that of models based on a discrete latent process following a Markov chain, while retaining a reduced number of parameters. The effect of different formulations of the latent structure of the model is evaluated in terms of estimates of the regression parameters for the covariates.	Quadrature methods,Latent Markov model,Hidden Markov model,Expectation-maximization algorithm,Proportional odds model	Bartolucci, Francesco@Univ Perugia, I-06123 Perugia, Italy::Bacci, Silvia@Univ Perugia, I-06123 Perugia, Italy::Pennoni, Fulvia@Univ Milano Bicocca, Milan, Italy
Flexible regression models over river networks	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12024	JAN 2014	41	Many statistical models are available for spatial data but the vast majority of these assume that spatial separation can be measured by Euclidean distance. Data which are collected over river networks constitute a notable and commonly occurring exception, where distance must be measured along complex paths and, in addition, account must be taken of the relative flows of water into and out of confluences. Suitable models for this type of data have been constructed based on covariance functions. The aim of the paper is to place the focus on underlying spatial trends by adopting a regression formulation and using methods which allow smooth but flexible patterns. Specifically, kernel methods and penalized splines are investigated, with the latter proving more suitable from both computational and modelling perspectives. In addition to their use in a purely spatial setting, penalized splines also offer a convenient route to the construction of spatiotemporal models, where data are available over time as well as over space. Models which include main effects and spatiotemporal interactions, as well as seasonal terms and interactions, are constructed for data on nitrate pollution in the River Tweed. The results give valuable insight into the changes in water quality in both space and time.	Flexible regression,Kernels,Network,Penalized splines,Smoothing,Spatial separation,Spatiotemporal models,Water quality	O'Donnell, David@Univ Glasgow, Glasgow G12 8QQ, Lanark, Scotland::Rushworth, Alastair@Univ Glasgow, Glasgow G12 8QQ, Lanark, Scotland::Bowman, Adrian W.@Univ Glasgow, Glasgow G12 8QQ, Lanark, Scotland::Scott, E. Marian@Univ Glasgow, Glasgow G12 8QQ, Lanark, Scotland::Hallard, Mark@Scottish Environm Protect Agcy, Stirling, Scotland
Time varying frailty models and the estimation of heterogeneities in transmission of infectious diseases	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12033	JAN 2014	28	A frailty modelling framework is presented for representing and making inference on individual heterogeneities that are relevant to the transmission of infectious diseases, including heterogeneities that evolve over time. Central to this framework is the use of multivariate data on several infections. We explore new simple but flexible families of time-dependent frailty models, in which the frailty is modulated over time in a deterministic fashion. Methods of estimation, issues of identifiability and model choice are discussed. Results from such models are interpreted in the light of concomitant information on routes of transmission. Applications to paired serological survey data on a range of infections with the same or different routes of transmission are presented.	Current status data,Frailty,Heterogeneity,Infectious diseases,Serological survey,Time varying frailty models,Transmission routes	Unkel, Steffen@Univ Giessen, D-35392 Giessen, Germany::Farrington, C. Paddy@Open Univ, Milton Keynes MK7 6AA, Bucks, England::Whitaker, Heather J.@Open Univ, Milton Keynes MK7 6AA, Bucks, England::Pebody, Richard@Publ Hlth England, London, England
Estimating disease onset distribution functions in mutation carriers with censored mixture data	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/rssc.12025	JAN 2014	32	We consider non-parametric estimation of disease onset distribution functions in multiple populations by using censored data with unknown population identifiers. The problem is motivated from studies aiming at estimating the age-specific disease risk distribution in deleterious mutation carriers for genetic counselling and design of therapeutic intervention trials to modify disease progression (i.e. to slow down the development of symptoms and to delay the onset of disease). In some of these studies, the distribution of disease risk in participants assumes a mixture form. Although the population identifiers are missing, study design and scientific knowledge allow calculation of the probability of a subject belonging to each population. We propose a general family of weighted least squares estimators and show that existing consistent non-parametric methods belong to this family. We identify a computationally effortless estimator in the family, study its asymptotic properties and show its significant gain in efficiency compared with the existing estimators in the literature. The application to a large genetic epidemiological study of Huntington's disease reveals information on the age-at-onset distribution of Huntington's disease which sheds light on some clinical hypotheses.	Huntington's disease,Mixture observations,Penetrance function,Risk prediction,Unknown population label	Ma, Yanyuan@Texas A&M Univ, College Stn, TX USA::Wang, Yuanjia@Columbia Univ, New York, NY 10032 USA
Ordinal latent variable models and their application in the study of newly licensed teenage drivers	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01065.x	2013	19	. In a unique longitudinal study of teen driving, risky driving behaviour and the occurrence of crashes or near crashes are measured prospectively over the first 18 months of licensure. Of scientific interest is relating the two processes and developing a predictor of crashes from previous risky driving behaviour. In this work, we propose two latent class models for relating risky driving behaviour to the occurrence of a crash or near-crash event. The first approach models the binary longitudinal crash or near-crash outcome by using a binary latent variable which depends on risky driving covariates and previous outcomes. A random-effects model introduces heterogeneity among subjects in modelling the mean value of the latent state. The second approach extends the first model to the ordinal case where the latent state is composed of K ordinal classes. Additionally, we discuss an alternative hidden Markov model formulation. Estimation is performed by using the expectationmaximization algorithm and Monte Carlo expectationmaximization. We illustrate the importance of using these latent class modelling approaches through the analysis of the teen driving behaviour.	Driving study,Latent class modelling,Monte Carlo expectationmaximization	Jackson, John C.@US Mil Acad, West Point, NY 10996 USA::Albert, Paul S.@Eunice Kennedy Shriver Natl Inst Child Hlth & Hum, Rockville, MD USA::Zhang, Zhiwei@Eunice Kennedy Shriver Natl Inst Child Hlth & Hum, Rockville, MD USA::Simons-Morton, Bruce@Eunice Kennedy Shriver Natl Inst Child Hlth & Hum, Rockville, MD USA
Estimating the relationship between women's education and fertility in Botswana by using an instrumental variable approach to semiparametric expectile regression	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01050.x	2013	50	We analyse the education-fertility relationship by using data on women from Botswana. A realistic quantification of such a relationship can be problematic for various reasons. First, factors such as motivation and ability are associated with fertility and education but cannot be observed and as a consequence cannot be included in the model. Here, the use of classical estimation methods will clearly result in inconsistent and biased parameter estimates. Second, there is strong heteroscedasticity in the data, which makes it very difficult to specify a suitable error distribution. Finally, covariate-response relationships can exhibit non-linear patterns. Provided that an instrumental variable is available, it is possible to employ a two-stage-type estimation approach to account for unobservable confounders. Such a technique is among the most widely used methods for isolating the effect of a predictor of interest in the presence of unobservable confounding and assures consistent estimation results. A two-stage approach can be embedded in a semiparametric expectile regression setting, hence providing possibilities for flexible additive covariate structures and modelling the whole conditional distribution of the response. Owing to its convenient estimation techniques, expectile regression may be preferable to quantile regression relying on linear programming techniques which require more numerical effort and may not accommodate very flexible model structures. We introduce a semiparametric instrumental variable expectile regression approach and study its empirical properties via an extensive simulation study. Further, corrected confidence intervals for the two-stage approach are presented. The methods are then employed to assess the education-fertility relationship.	Expectiles,Instrumental variable,Least asymmetric weighted squares,P-splines,Two-stage estimation approach	Sobotka, Fabian@Carl von Ossietzky Univ Oldenburg, D-26111 Oldenburg, Germany::Radice, Rosalba@London Sch Hyg & Trop Med, London WC1, England::Marra, Giampiero@UCL, London WC1E 6BT, England::Kneib, Thomas@Univ Gottingen, Gottingen, Germany
A likelihood-based sensitivity analysis for publication bias in meta-analysis	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01049.x	2013	33	A common conjecture in the study of publication bias is that studies reporting a significant result are more likely to be selected for review than studies whose results are inconclusive. We envisage a population of studies following the standard random-effects model of meta-analysis, and a selection probability given by a function of the study's 't-statistic'. In practice it is difficult to estimate this function, and hence difficult to estimate its associated bias correction. The paper suggests the more modest aim of a sensitivity analysis in which the treatment effect is estimated by maximum likelihood constrained by given values of the marginal probability of selection. This gives a graphical summary of how the inference from a meta-analysis changes as we allow for increasing selection (as the marginal selection probability decreases from 1), with an associated diagnostic plot comparing the observed treatment effects with their fitted values implied by the corresponding selection model. The approach is motivated by a medical example in which the highly significant result of a published meta-analysis was subsequently overturned by the results of a large-scale clinical trial.	Meta-analysis,Publication bias,Selection bias,Selection model,Sensitivity analysis	Copas, John B.@Univ Warwick, Coventry CV4 7AL, W Midlands, England
Evaluating joint effects of induction-salvage treatment regimes on overall survival in acute leukaemia	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01048.x	2013	24	Typical oncology practice often includes not only an initial front-line treatment but also subsequent treatments given if the initial treatment fails. The physician chooses a treatment at each stage based on the patient's baseline covariates and history of previous treatments and outcomes. Such sequentially adaptive medical decision-making processes are known as dynamic treatment regimes, treatment policies ormultistage adaptive treatment strategies. Conventional analyses in terms of front-line treatments that ignore subsequent treatments may be misleading, because they actually are an evaluation of more than front-line treatment effects on outcome. We are motivated by data from a randomized trial of four combination chemotherapies given as front-line treatments to patients with acute leukaemia. Most patients in the trial also received a second-line treatment, which was chosen adaptively and subjectively rather than by randomization, either because the initial treatment was ineffective or the patient's cancer later recurred. We evaluate effects on overall survival time of the 16 two-stage strategies that actually were used. Our methods include a likelihood-based regression approach in which the transition times of all possible multistage outcome paths are modelled, and estimating equations with inverse probability of treatment weighting to correct for bias. Although the two approaches give different numerical estimates of mean survival time, they lead to the same substantive conclusions when comparing the two-stage regimes.	Causal inference,Clinical trial,Dynamic treatment regime,Treatment policy	Wahed, Abdus S.@Univ Pittsburgh, Pittsburgh, PA 15261 USA::Thall, Peter F.@Univ Texas MD Anderson Canc Ctr, Houston, TX 77030 USA
General classes of multiple binary regression models in dose finding problems for combination therapies	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01054.x	2013	13	The dose finding problem for single compounds has been generalized to combination therapies and several binary regression models have been proposed in the literature. We propose general tools to evaluate them, in particular dose-free parameterizations of the risks of toxicity and risk ratio functions and plots. New classes of risk functions are also proposed. They generate low dimensional parametric risk functions which are made available to the researcher and commented on for their clinical relevance.	Continual reassessment method,Copulas,Dose-free parameterization	Gasparini, Mauro@Politecn Torino, I-10129 Turin, Italy
Dynamic Bradley-Terry modelling of sports tournaments	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01046.x	2013	32	In the course of national sports tournaments, usually lasting several months, it is expected that the abilities of teams taking part in the tournament will change over time. A dynamic extension of the Bradley-Terry model for paired comparison data is introduced to model the outcomes of sporting contests, allowing for time varying abilities. It is assumed that teams' home and away abilities depend on past results through exponentially weighted moving average processes. The model proposed is applied to sports data with and without tied contests, namely the 2009-2010 regular season of the National Basketball Association tournament and the 2008-2009 Italian Serie A football season.	Bradley-Terry model,Cumulative logit model,Exponentially weighted moving average process,Paired comparisons,Sports tournaments	Cattelan, Manuela@Univ Padua, I-35121 Padua, Italy::Varin, Cristiano@Univ Ca Foscari, Venice, Italy::Firth, David@Univ Warwick, Coventry CV4 7AL, W Midlands, England
Fast linked analyses for scenario-based hierarchies	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01042.x	2012	35	. When using computer models to provide policy support it is normal to encounter ensembles that test only a handful of feasible or idealized decision scenarios. We present a new methodology for performing multilevel emulation of a complex model as a function of any decision within a predefined class that makes specific use of a scenario ensemble of opportunity on a fast or early version of a simulator and a small, well-chosen, design on our current simulator of interest. The method exploits a geometrical approach to Bayesian inference and is designed to be fast, to facilitate detailed diagnostic checking of our emulators by allowing us to carry out many analyses very quickly. Our motivating application involved constructing an emulator for the UK Met Office Hadley Centre coupled climate model HadCM3 as a function of carbon dioxide forcing, which was part of a RAPID programme deliverable to the UK Met Office funded by the Natural Environment Research Council. Our application involved severe time pressure as well as limited access to runs of HadCM3 and a scenario ensemble of opportunity on a lower resolution version of the model.	Bayesian analysis,Computer models,Emulation,Policy support,Restricted inner product space,Scenario analysis	Williamson, Daniel@Univ Durham, Dept Math Sci, Durham DH1 3LE, England::Goldstein, Michael); Blaker, A (Blaker, Adam@Natl Oceanog Ctr, Southampton, Hants, England
Smooth principal components for investigating changes in covariances over time	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01037.x	2012	38	. The complex interrelated nature of multivariate systems can result in relationships and covariance structures that change over time. Smooth principal components analysis is proposed as a means of investigating whether and how the covariance structure of multiple response variables changes over time, after removing a smooth function for the mean, and this is motivated and illustrated by using data from an aircraft technology study and a lake ecosystem. Inferential procedures are investigated in the cases of independent and dependent errors, with a bootstrapping procedure proposed to detect changes in the direction or variance of components.	Bootstrapping,Principal components analysis,Smoothing,Time	Miller, Claire@Univ Glasgow, Sch Math & Stat, Glasgow G12 8QW, Lanark, Scotland
The effect of late onset, short-term caloric restriction on the core temperature and physical activity in mice	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2012.01045.x	2012	33	. Caloric restriction (CR) has been shown to delay the onset of cancer and other diseases that are associated with aging. Currently there are very few studies examining the whole-animal physiological response to late onset CR. We study the ways by which mice physiologically compensate for reduced availability of food given exposure to late onset CR and compare these with mice fed ad libitum. The data arise from a 70-day experiment that was undertaken by the authors. A joint model is developed to describe core body temperature and levels of activity and the model parameters assessed for temporal dependence by using dynamic linear models. A Bayesian approach is used throughout. The dynamic parameters are shown to be plausibly constant over time and the constant parameter model is then embedded within a random-effects structure to explore differences between the responses of CR and ad libitum mice. This model is shown to provide a good description of the data.	Core body temperature,Dynamic linear model,Mice,Physical activity,Random effects	Golightly, Andrew@Newcastle Univ, Sch Math & Stat, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England
Spatiotemporal quantile regression for detecting distributional changes in environmental processes	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01025.x	2012	35	. Climate change may lead to changes in several aspects of the distribution of climate variables, including changes in the mean, increased variability and severity of extreme events. We propose the use of spatiotemporal quantile regression as a flexible and interpretable method for simultaneously detecting changes in several features of the distribution of climate variables. The spatiotemporal quantile regression model assumes that each quantile level changes linearly in time, permitting straightforward inference on the time trend for each quantile level. Unlike classical quantile regression which uses model-free methods to analyse a single quantile or several quantiles separately, we take a model-based approach which jointly models all quantiles, and thus the entire response distribution. In the spatiotemporal quantile regression model, each spatial location has its own quantile function that evolves over time, and the quantile functions are smoothed spatially by using Gaussian process priors. We propose a basis expansion for the quantile function that permits a closed form for the likelihood and allows for residual correlation modelling via a Gaussian spatial copula. We illustrate the methods by using temperature data for the south-east USA from the years 19312009. For these data, borrowing information across space identifies more significant time trends than classical non-spatial quantile regression. We find a decreasing time trend for much of the spatial domain for monthly mean and maximum temperatures. For the lower quantiles of monthly minimum temperature, we find a decrease in Georgia and Florida, and an increase in Virginia and the Carolinas.	Bayesian hierarchical model,Climate change,Non-Gaussian data,US temperature data,Warming hole	
Group testing in heterogeneous populations by using halving algorithms	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01008.x	2012	20	. Group (pooled) testing is often used to reduce the total number of tests that are needed to screen a large number of individuals for an infectious disease or some other binary characteristic. Traditionally, research in group testing has assumed that each individual is independent with the same risk of positivity. More recently, there has been a growing set of literature generalizing previous work in group testing to include heterogeneous populations so that each individual has a different risk of positivity. We investigate the effect of acknowledging population heterogeneity on a commonly used group testing procedure which is known as halving. For this procedure, positive groups are successively split into two equal-sized halves until all groups test negatively or until individual testing occurs. We show that heterogeneity does not affect the mean number of tests when individuals are randomly assigned to subgroups. However, when individuals are assigned to subgroups on the basis of their risk probabilities, we show that our proposed procedures reduce the number of tests by taking advantage of the heterogeneity. This is illustrated by using chlamydia and gonorrhoea screening data from the state of Nebraska.	Binary response,Classification,Identification,Pooled testing,Retesting,Screening	Black, Michael S.); Bilder, CR (Bilder, Christopher R.@Univ Nebraska, Dept Stat, Lincoln, NE 68583 USA::Tebbs, Joshua M.@Univ S Carolina, Columbia, SC 29208 USA
Combining outputs from the North American Regional Climate Change Assessment Program by using a Bayesian hierarchical model	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01010.x	2012	44	. We investigate the 20-year-average boreal winter temperatures generated by an ensemble of six regional climate models (RCMs) in phase I of the North American Regional Climate Change Assessment Program. We use the long-run average (20-year integration) to smooth out variability and to capture the climate properties from the RCM outputs. We find that, although the RCMs capture the large-scale climate variation from coast to coast and from south to north similarly, their outputs can differ substantially in some regions. We propose a Bayesian hierarchical model to synthesize information from the ensemble of RCMs, and we construct a consensus climate signal with each RCM contributing to the consensus according to its own variability parameter. The Bayesian methodology enables us to make posterior inference on all the unknowns, including the large-scale fixed effects and the small-scale random effects in the consensus climate signal and in each RCM. The joint distributions of the consensus climate and the outputs from the RCMs are also investigated through posterior means, posterior variances and posterior spatial quantiles. We use a spatial random-effects model in the Bayesian hierarchical model and, consequently, we can deal with the large data sets of fine resolution outputs from all the RCMs. Additionally, our model allows a flexible spatial covariance structure without assuming stationarity or isotropy.	Downscaling,North American Regional Climate Change Assessment Program,Posterior distribution,Regional climate model,Spatial random-effects model	Kang, Emily L.@Univ Cincinnati, Cincinnati, OH 45221 USA::Cressie, Noel@Ohio State Univ, Columbus, OH 43210 USA::Sain, Stephan R.@Natl Ctr Atmospher Res, Boulder, CO 80307 USA
A versatile method for confirmatory evaluation of the effects of a covariate in multiple models	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01005.x	2012	25	. Modern epidemiology often requires testing of the effect of a covariate on multiple end points from the same study. However, popular state of the art methods for multiple testing require the tests to be evaluated within the framework of a single model unifying all end points. This severely limits their use in applications where there are different types of end point, e.g. binary, continuous or time to event. We use an asymptotic representation of parameter estimates to combine multiple models without additional constraints. This result enables the use of established tools for multiple testing to provide a fine-tuned control of the overall type I error in a wide range of epidemiological experiments where in reality no other useful alternative exists. The methodology proposed is applied to a multiple-end-point study of the effect of neonatal bacterial colonization on development of childhood asthma.	Epidemiology,Multiple end points,Multiple models,Multiple testing,Type I error	Pipper, Christian Bressen@Univ Copenhagen, Stat Grp, Dept Basic Sci & Environm, Fac Life Sci, DK-1871 Frederiksberg C, Denmark
Variable selection for high dimensional Bayesian density estimation: application to human exposure simulation	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.00772.x	2012	34	. Numerous studies have linked ambient air pollution and adverse health outcomes. Many studies of this nature relate outdoor pollution levels measured at a few monitoring stations with health outcomes. Recently, computational methods have been developed to model the distribution of personal exposures, rather than ambient concentration, and then relate the exposure distribution to the health outcome. Although these methods show great promise, they are limited by the computational demands of the exposure model. We propose a method to alleviate these computational burdens with the eventual goal of implementing a national study of the health effects of air pollution exposure. Our approach is to develop a statistical emulator for the exposure model, i.e. we use Bayesian density estimation to predict the conditional exposure distribution as a function of several variables, such as temperature, human activity and physical characteristics of the pollutant. This poses a challenging statistical problem because there are many predictors of the exposure distribution and density estimation is notoriously difficult in high dimensions. To overcome this challenge, we use stochastic search variable selection to identify a subset of the variables that have more than just additive effects on the mean of the exposure distribution. We apply our method to emulate an ozone exposure model in Philadelphia.	Air pollution,Bayesian non-parametrics,High dimensional data,Kernel stick breaking prior,Stochastic computer models	Reich, Brian J.@N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA::Kalendra, Eric); Storlie, CB (Storlie, Curtis B.@Los Alamos Natl Lab, Los Alamos, NM 87545 USA
Spatial modelling of lupus incidence over 40 years with changes in census areas	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01004.x	2012	19	. Clinical data on the location of residence at the time of diagnosis of new lupus cases in Toronto, Canada, for the 40 years to 2007 are modelled with the aim of finding areas of abnormally high risk. Inference is complicated by numerous irregular changes in the census regions on which population is reported. A model is introduced consisting of a continuous random spatial surface and fixed effects for time and ages of individuals. The process is modelled on a fine grid and Bayesian inference performed by using integrated nested Laplace approximations. Predicted risk surfaces and posterior probabilities of exceedance are produced for lupus and, for comparison, psoriatic arthritis data from the same clinic. Simulations studies are also carried out to understand better the performance of the model proposed as well as to compare with existing methods.	Integrated nested Laplace approximation,Bayesian inference,Changing boundaries,Disease mapping	Li, Ye@Univ Toronto, Sch Publ Hlth, Toronto, ON M5T 3M7, Canada::Brown, Patrick@Canc Care Ontario, Toronto, ON, Canada::Rue, Havard@Norwegian Univ Sci & Technol, N-7034 Trondheim, Norway::al-Maini, Mustafa@Mafraq Hosp, Abu Dhabi, U Arab Emirates
Tracking measles infection through non-linear state space models	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01001.x	2012	24	. Estimating the burden of infectious disease is complicated by the general tendency for underreporting of cases. When the reporting rate is unknown, conventional methods have relied on accounting methods that do not make explicit use of surveillance data or the temporal dynamics of transmission and infection. State space models are a framework for various methods that allow dynamic models to be fitted with partially or imperfectly observed surveillance data. State space models are an appealing approach to burden estimation as they combine expert knowledge in the form of an underlying dynamic model but make explicit use of surveillance data to estimate parameter values, to predict unobserved elements of the model and to provide standard errors for estimates.	Burden estimation,Disease surveillance,Extended Kalman filter,Susceptible-infected-recovered model,Under-reporting	Chen, Shi); Fricks, J (Fricks, John); Ferrari, MJ (Ferrari, Matthew J.@Penn State Univ, Ctr Dis Dynam, University Pk, PA 16802 USA
Borrowing information across populations in estimating positive and negative predictive values	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.00761.x	2011	13	A marker's capacity to predict the risk of a disease depends on the prevalence of disease in the target population and its accuracy of classification, i.e. its ability to discriminate diseased subjects from non-diseased subjects. The latter is often considered an intrinsic property of the marker; it is independent of disease prevalence and hence more likely to be similar across populations than risk prediction measures. In this paper, we are interested in evaluating the population-specific performance of a risk prediction marker in terms of the positive predictive value PPV and negative predictive value NPV at given thresholds, when samples are available from the target population as well as from another population. A default strategy is to estimate PPV and NPV using samples from the target population only. However, when the marker's accuracy of classification as characterized by a specific point on the receiver operating characteristics curve is similar across populations, borrowing information across populations allows increased efficiency in estimating PPV and NPV. We develop estimators that optimally combine information across populations. We apply this methodology to a cross-sectional study where we evaluate PCA3 as a risk prediction marker for prostate cancer among subjects with or without a previous negative biopsy.	Biomarker,Classification,Negative predictive value,Positive predictive value,Sensitivity,Specificity	Huang, Ying@Fred Hutchinson Canc Res Ctr, Dept Vaccine & Infect Dis & Publ Hlth Sci, Seattle, WA 98109 USA::Fong, Youyi); Wei, J (Wei, John@Univ Michigan, Ann Arbor, MI 48109 USA
A case-study in the clinical epidemiology of psoriatic arthritis: multistate models and causal arguments	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01021.x	2011	60	In psoriatic arthritis, permanent joint damage characterizes disease progression and represents a major debilitating aspect of the disease. Understanding the process of joint damage will assist in the treatment and disease management of patients. Multistate models provide a means to examine patterns of disease, such as symmetric joint damage. Additionally, the link between damage and the dynamic course of disease activity (represented by joint swelling and stress pain) at both the individual joint level and otherwise can be represented within a correlated multistate model framework. Correlation is reflected through the use of random effects for progressive models and robust variance estimation for non-progressive models. Such analyses, undertaken with data from a large psoriatic arthritis cohort, are discussed and the extent to which they permit causal reasoning is considered. For this, emphasis is given to the use of the Bradford Hill criteria for causation in observational studies and the concept of local (in) dependence to capture the dynamic nature of the relationships.	Bradford Hill criteria,Causality,Composable Markov process,Damage,Disease activity,Dynamic modelling,Granger causality and non-causality,Interval censoring,Local dependence and independence,Multistate models,Psoriatic arthritis,Random effect,Robust information sandwich estimator,Temporality	O'Keeffe, Aidan G.@Univ Forvie Site, MRC, Biostat Unit, Cambridge CB2 0SR, England::Tom, Brian D. M.@Univ Forvie Site, MRC, Biostat Unit, Cambridge CB2 0SR, England::Farewell, Vernon T.@Univ Forvie Site, MRC, Biostat Unit, Cambridge CB2 0SR, England
New findings from terrorism data: Dirichlet process random-effects models for latent groups	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01022.x	2011	56	Data obtained describing terrorist events are particularly difficult to analyse, owing to the many problems that are associated with the data collection process, the inherent variability in the data themselves and the usually poor level of measurement coming from observing political actors who seek not to provide reliable data on their activities. Thus, there is a need for sophisticated modelling to obtain reasonable inferences from these data. Here we develop a logistic random-effects specification using a Dirichlet process to model the random effects. We first look at how such a model can best be implemented, and then we use the model to analyse terrorism data. We see that the richer Dirichlet process random-effects model, compared with a normal random-effects model, can remove more of the underlying variability from the data, uncovering latent information that would not otherwise have been revealed.	Empirical studies of terrorism,Generalized linear mixed models,Gibbs sampling,Hierarchical models,Logistic regression,Metropolis-Hastings algorithm,Terrorists	Kyung, Minjung@Washington Univ, St Louis, MO USA::Gill, Jeff@Washington Univ, St Louis, MO USA::Casella, George@Univ Florida, Dept Stat, Gainesville, FL 32611 USA
Capture-recapture estimation by means of empirical Bayesian smoothing with an application to the geographical distribution of hidden scrapie in Great Britain	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2011.01018.x	2011	29	The paper discusses population size estimation on the basis of a frequency distribution of zero-truncated counts and is motivated by a study on the geographical distribution of hidden scrapie in Great Britain. Aggregation of scrapie cases is considered at the county level and results in sparse zero-truncated count distributions which make the application of conventional capture-recapture procedures for estimating the hidden part of the scrapie-affected population difficult. We suggest a smoothed generalization of Zelterman's estimator of population size which overcomes the overestimation bias of the conventional Zelterman estimator and instead produces a lower bound, which is typically larger than Chao's lower bound estimator. The estimator uses an empirical Bayes approach with various choices for the prior distribution including a parametric choice of the gamma distribution as well as various non-parametric distributions. A simulation study investigates the performance of the new estimators, and also in comparison with conventional estimators. The empirical Bayes estimator with a non-parametric mixture model as prior performs well and the boundary problem of the conventional non-parametric discrete mixture model estimator leading to spurious population size is avoided. In the application to hidden scrapie in Great Britain the new estimators lead to maps of scrapie of observed-hidden ratios as well as completeness of the current surveillance system.	Capture-recapture,Empirical Bayes methods,Geographical analysis,Non-parametric mixture model	Boehning, Dankmar@Univ Reading, Dept Math & Stat, Sch Math & Phys Sci, Reading RG6 6BX, Berks, England::Kuhnert, Ronny@Robert Koch Inst, D-1000 Berlin, Germany::Vilas, Victor Del Rio@Dept Environm Food & Rural Affairs, London, England
Mixture modelling as an exploratory framework for genotype-trait associations	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00750.x	2011	33	We propose a mixture modelling framework for both identifying and exploring the nature of genotype-trait associations. This framework extends the classical mixed effects modelling approach for this setting by incorporating a Gaussian mixture distribution for random genotype effects. The primary advantages of this paradigm over existing approaches include that the mixture modelling framework addresses the degrees-of-freedom challenge that is inherent in application of the usual fixed effects analysis of covariance, relaxes the restrictive single normal distribution assumption of the classical mixed effects models and offers an exploratory framework for discovery of underlying structure across multiple genetic loci. An application to data arising from a study of antiretroviral-associated dyslipidaemia in human immunodeficiency virus infection is presented. Extensive simulations studies are also implemented to investigate the performance of this approach.	Genetic associations,Latent class,Mixture models	Au, Kinman); Lin, RH (Lin, Rongheng); Foulkes, AS (Foulkes, Andrea S.@Univ Massachusetts, Div Biostat & Epidemiol, Amherst, MA 01003 USA
Assessing similarity of DNA profiles	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00742.x	2011	6	The genetic similarity of strains of a pathogen can be assessed by using a matrix of dissimilarities that is derived from bands in their DNA profile which are present or absent. The dependence between elements of the dissimilarity matrix, if not accounted for, results in underestimation of the variance in comparisons between groups of strains which are differentiated according to the possession of an attribute. We examine a previously proposed statistic for determining whether a group of strains is more similar than expected. We show the limitations of this statistic and propose a new statistic which better addresses the hypotheses that are usually considered in this field of study. The statistic proposed is based on similarity between strains within the group of interest and with those outside. This statistic also needs to account for the dependence in the raw data, and we use the correlation between elements of the dissimilarity matrix to investigate how this dependence affects the underestimation of the variance. Using examples involving the pathogenic yeast Candida, we show how permutation tests can be applied to the differentiation of groups of strains.	Dependence,DNA profile,Permutation test,Similarity,Variance inflation factor	Hepworth, Graham@Univ Melbourne, Ctr Stat Consulting, Melbourne, Vic 3010, Australia
Using integrated nested Laplace approximations for the evaluation of veterinary surveillance data from Switzerland: a case-study	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00740.x	2011	36	Spatiotemporal disease mapping models have been used extensively to describe the pattern of surveillance data. They are usually formulated in a hierarchical Bayesian framework and posterior marginals are not available in closed form. Hence, the standard method for parameter estimation is Markov chain Monte Carlo algorithms. A new method for approximate Bayesian inference in latent Gaussian models using integrated nested Laplace approximations has recently been proposed as an alternative. This approach promises very precise results in short computational time. The aim of the paper is to show how integrated nested Laplace approximations can be used as an inferential tool for a variety of spatiotemporal models for the analysis of reported cases of bovine viral diarrhoea in cattle from Switzerland. Conclusions concerning the problem of under-reporting in the data are drawn via a multilevel modelling strategy. Furthermore, a comparison with Markov chain Monte Carlo methods with regard to the accuracy of the parameter estimates and the usability of both approaches in practice is conducted. Approaches to model choice using integrated nested Laplace approximations are also presented.	Disease mapping,Integrated nested Laplace approximations,Leave-one-out cross-validation,Spatiotemporal models	Schroedle, Birgit@Univ Zurich, Inst Social & Prevent Med, Biostat Unit, CH-8006 Zurich, Switzerland::Held, Leonhard); Riebler, A (Riebler, Andrea); Danuser, J (Danuser, Juerg@Fed Vet Off, Bern, Switzerland
Approximate Bayesian computation using indirect inference	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00747.x	2011	24	We present a novel approach for developing summary statistics for use in approximate Bayesian computation (ABC) algorithms by using indirect inference. ABC methods are useful for posterior inference in the presence of an intractable likelihood function. In the indirect inference approach to ABC the parameters of an auxiliary model fitted to the data become the summary statistics. Although applicable to any ABC technique, we embed this approach within a sequential Monte Carlo algorithm that is completely adaptive and requires very little tuning. This methodological development was motivated by an application involving data on macroparasite population evolution modelled by a trivariate stochastic process for which there is no tractable likelihood function. The auxiliary model here is based on a beta-binomial distribution. The main objective of the analysis is to determine which parameters of the stochastic model are estimable from the observed data on mature parasite worms.	Approximate Bayesian computation,Beta-binomial model,Indirect inference,Macroparasite,Markov process,Sequential Monte Carlo methods	Drovandi, Christopher C.@Queensland Univ Technol, Brisbane, Qld 4001, Australia::Pettitt, Anthony N.@Queensland Univ Technol, Brisbane, Qld 4001, Australia::Faddy, Malcolm J.@Queensland Univ Technol, Brisbane, Qld 4001, Australia
Multivariate non-linear time series modelling of exposure and risk in road safety research	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00690.x	2010	20	A multivariate non-linear time series model for road safety data is presented. The model is applied in a case-study into the development of a yearly time series of numbers of fatal accidents (inside and outside urban areas) and numbers of kilometres driven by motor vehicles in the Netherlands between 1961 and 2000. The model accounts for missing entries in the disaggregated numbers of kilometres driven although the aggregated numbers are observed throughout. We consider a multivariate non-linear time series model for the analysis of these data. The model consists of dynamic unobserved factors for exposure and risk that are related in a non-linear way to the number of fatal accidents. The multivariate dimension of the model is due to its inclusion of multiple time series for inside and outside urban areas. Approximate maximum likelihood methods based on the extended Kalman filter are utilized for the estimation of unknown parameters. The latent factors are estimated by extended smoothing methods. It is concluded that the salient features of the observed time series are captured by the model in a satisfactory way.	Approximate maximum likelihood,Extended Kalman filter,Missing data,Road casualties,State space model,Unobserved factors	Bijleveld, Frits@Inst Rd Safety Res, Leidschendam, Netherlands::Commandeur, Jacques@Inst Rd Safety Res, Leidschendam, Netherlands::Koopman, Siem Jan); van Montfort, K (van Montfort, Kees@Vrije Univ Amsterdam, Dept Econometr, NL-1081 HV Amsterdam, Netherlands
Estimating infectious disease parameters from data on social contacts and serological status	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00693.x	2010	32	In dynamic models of infectious disease transmission, typically various mixing patterns are imposed on the so-called 'who acquires infection from whom' matrix. These imposed mixing patterns are based on prior knowledge of age-related social mixing behaviour rather than observations. Alternatively, we can assume that transmission rates for infections transmitted predominantly through non-sexual social contacts are proportional to rates of conversational contact which can be estimated from a contact survey. In general, however, contacts reported in social contact surveys are proxies of those events by which transmission may occur and there may be age-specific characteristics that are related to susceptibility and infectiousness which are not captured by the contact rates. Therefore, we model transmission as the product of two age-specific variables: the age-specific contact rate and an age-specific proportionality factor, which entails an improvement of fit for the seroprevalence of the varicella zoster virus in Belgium. Furthermore, we address the effect on the estimation of the basic reproduction number, using non-parametric bootstrapping to account for different sources of variability and using multimodel inference to deal with model selection uncertainty. The method proposed makes it possible to obtain important information on transmission dynamics that cannot be inferred from approaches that have been traditionally applied hitherto.	Basic reproduction number,Bootstrap procedure,Model averageing,Model selection,Social contact data,Transmission parameters,Who acquires infection from whom matrix	Goeyvaerts, Nele@Hasselt Univ, Interuniv Inst Biostat & Stat Bioinformat, B-3590 Diepenbeek, Belgium::Hens, Niel@Univ Antwerp, Antwerp, Belgium::Ogunjimi, Benson@Univ Antwerp, Antwerp, Belgium::Aerts, Marc); Shkedy, Z (Shkedy, Ziv); Van Damme, P (Van Damme, Pierre@Univ Antwerp, Antwerp, Belgium::Beutels, Philippe@Univ Antwerp, Antwerp, Belgium
A framework for interpreting climate model outputs	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00694.x	2010	44	Projections of future climate are often based on deterministic models of the Earth's atmosphere and oceans. However, these projections can vary widely between models, with differences becoming more pronounced at the relatively fine spatial and temporal scales that are relevant in many applications. We suggest that the resulting uncertainty can be handled in a logically coherent and interpretable way by using a hierarchical statistical model, implemented in a Bayesian framework. Model fitting using Markov chain Monte Carlo techniques is feasible but moderately time consuming; the computational efficiency can, however, be improved dramatically by substituting maximum likelihood estimates for the original data. The work was motivated by the need for future precipitation scenarios in the UK, in applications such as flood risk assessment and water resource management. We illustrate the methodology by considering the generation of multivariate time series of atmospheric variables, that can be used to drive stochastic simulations of high resolution precipitation for risk assessment purposes.	Climate change,Climate model uncertainty,Contemporaneous auto-regressive moving average models,Downscaling,Multimodel ensembles,Sufficient statistics	Leith, Nadja A.); Chandler, RE (Chandler, Richard E.@UCL, Dept Stat Sci, London WC1E 6BT, England
Racial disparities in risks of mortality in a sample of the US Medicare population	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00691.x	2010	47	Racial disparities in risks of mortality adjusted for socio-economic status are not well understood. To add to the understanding of racial disparities, we construct and analyse a data set that links, at individual and zip code levels, three government databases: Medicare, the Medicare Current Beneficiary Survey and US census. Our study population includes more than 4 million Medicare enrollees residing in 2095 zip codes in the north-east region of the USA. We develop hierarchical models to estimate the black-white disparities in risk of death, adjusted for both individual level and zip code level income. We define the population level attributable risk AR, relative attributable risk RAR and odds ratio OR of death comparing blacks versus whites, and we estimate these parameters by using a Bayesian approach via Markov chain Monte Carlo sampling. By applying the multiple-imputation method to fill in missing data, our estimates account for the uncertainty from the missing individual level income data. Results show that, for the Medicare population being studied, there is a statistically and substantively significantly higher risk of death for blacks compared with whites, in terms of all three measures AR, RAR and OR, both adjusted and not adjusted for income. In addition, after adjusting for income we find a statistically significant reduction in AR but not in RAR and OR.	Hierarchical model,Markov chain Monte Carlo methods,Multiple imputation,Racial disparity,Socio-economic status	Zhou, Yijie@Merck Res Labs, Rahway, NJ 07065 USA::Dominici, Francesca@Harvard Univ, Boston, MA 02115 USA::Louis, Thomas A.@Johns Hopkins Univ, Baltimore, MD USA
Bayesian inference for generalized stochastic population growth models with application to aphids	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00696.x	2010	28	We analyse the effects of various treatments on cotton aphids (Aphis gossypii). The standard analysis of count data on cotton aphids determines parameter values by assuming a deterministic growth model and combines these with the corresponding stochastic model to make predictions on population sizes, depending on treatment. Here, we use an integrated stochastic model to capture the intrinsic stochasticity, of both observed aphid counts and unobserved cumulative population size for all treatment combinations simultaneously. Unlike previous approaches, this allows us to explore explicitly and more accurately to assess treatment interactions. Markov chain Monte Carlo methods are used within a Bayesian framework to integrate over uncertainty that is associated with the unobserved cumulative population size and estimate parameters. We restrict attention to data on aphid counts in the Texas High Plains obtained for three different levels of irrigation water, nitrogen fertilizer and block, but we note that the methods that we develop can be applied to a wide range of problems in population ecology.	Cotton aphid,Markov chain Monte Carlo methods,Markov jump process,Moment closure approximation	Gillespie, Colin S.@Univ Newcastle, Sch Math & Stat, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England
Interval-censored data with repeated measurements and a cured subgroup	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	NO DOI	2010	23	The hypobaric decompression sickness data study was conducted by the National Aeronautics and Space Administration to investigate the risk of decompression sickness in hypobaric environments. The quantity of interest is the time to onset of grade IV venous gas emboli, which was mixed case interval censored because of measurement limitations. In the study, some subjects participated in multiple experiments, leading to repeated and correlated measurements on those subjects. In addition, it has been suggested that some subjects had a much lower risk of developing grade IV venous gas emboli than others, i.e. those subjects were immune from the event of interest (or 'cured'). We propose to use two-part models, where the first part describes the probability of cure and the second part describes the survival for susceptible subjects. We use two random effects to account for the correlated nature of measurements. A leverage bootstrap approach is proposed for model diagnosis. A simulation study shows satisfactory performance of the estimation and diagnosis approaches proposed. Model estimation and evaluation of the hypobaric decompression sickness data are carefully investigated.	Cure model,Interval censoring,Two-part model	Li, Jialiang@Natl Univ Singapore, Singapore 117548, Singapore::Ma, Shuangge@Yale Univ, Dept Epidemiol & Publ Hlth, New Haven, CT 06510 USA
Predicting snow velocity in large chute flows under different environmental conditions	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00717.x	2010	28	Observations, model evaluations and expert judgements are combined to make predictions of snow velocity in large chute experiments. Different experimental variables, namely the environmental conditions snow density and snow surface temperature, affect all aspects of this inference. We show how the effect of these two variables can be incorporated in our judgements regarding the uncertain parameters of the physical model, the discrepancy between the physical model and reality and the observation error. We adopt a Bayes linear approach to avoid the necessity of fully probabilistic belief specifications and demonstrate visual tools for statistical validation. Our results represent an important first step in improving the specification of uncertainty in model-based avalanche hazard mapping.	Avalanche,Bayes linear approach,Computer experiment,Herschel-Bulkley rheology,Model discrepancy	Rougier, Jonathan@Univ Bristol, Dept Math, Bristol BS8 1TW, Avon, England::Kern, Martin@BFW Inst Nat Hazards & Alpine Timberline, Innsbruck, Austria
Modelling long-term human immunodeficiency virus dynamic models with application to acquired immune deficiency syndrome clinical study	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00730.x	2010	22	Mathematical modelling of human immunodeficiency virus (HIV) dynamics has played an important role in acquired immune deficiency syndrome research. Deterministic dynamic models have been developed to study the viral dynamic process for understanding the pathogenesis of HIV type 1 infection and antiviral treatment strategies. We propose a new multistage estimation procedure which uses data, HIV viral load and CD4+ T-cell counts, from an acquired immune deficiency syndrome clinical study, to estimate the parameters in a long-term HIV dynamic model containing both constant and time varying parameters. Simulation studies and a real data application show that the methods proposed are efficient and appropriate to estimate both constant and time varying parameters in long-term HIV dynamic models.	Dynamic parameters,Human immunodeficiency virus dynamic systems,Multistage non-parametric and parametric techniques,State variables	
An application of hidden Markov models to the French variant Creutzfeldt-Jakob disease epidemic	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00714.x	2010	36	In 1996, the discovery of variant Creutzfeldt-Jakob disease in the UK raised serious concerns about a large-scale epidemic. These concerns have been heightened by the recent discovery of people in Britain who were infected through blood transfusion. The outbreak of variant Creutzfeldt-Jakob disease in France emerged more recently with 23 cases observed to date. We use a hidden Markov model to predict the scale of the epidemic in France. As accurate data on the most important epidemiological parameters are scarce, we incorporate estimates from previous studies. Parameter estimation is performed by using Markov chain Monte Carlo methods from which credible intervals for our predictions are obtained. The sensitivity of these predictions to important assumptions regarding population exposure is assessed.	Hidden Markov model,Markov chain Monte Carlo methods,Rare disease,Variant Creutzfeldt,Jakob disease	Chadeau-Hyam, Marc@Univ London Imperial Coll Sci Technol & Med, Sch Publ Hlth, Dept Epidemiol & Biostat, London W2 1PG, England::Clarke, Paul S.@Univ Bristol, Bristol BS8 1TH, Avon, England::Guihenneuc-Jouyaux, Chantal@Univ Paris 05, Villejuif, France::Cousens, Simon N.@London Sch Hyg & Trop Med, London, England::Will, Robert G.@Western Gen Hosp, Edinburgh EH4 2XU, Midlothian, Scotland::Ghani, Azra C.@Univ London Imperial Coll Sci Technol & Med, London, England
Bayesian quantile regression for count data with application to environmental epidemiology	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2010.00725.x	2010	25	Quantile regression estimates the relationship between covariates and the tau th quantile of the response distribution, rather than the mean. We present a Bayesian quantile regression model for count data and apply it in the field of environmental epidemiology, which is an area in which quantile regression is yet to be used. Our methods are applied to a new study of the relationship between long-term exposure to air pollution and respiratory hospital admissions in Scotland. We observe a decreasing relationship between pollution and the tau th quantile of the response distribution, with a relative risk ranging between 1.023 and 1.070.	Air pollution,Bayesian methods,Health effects,Quantile regression	Lee, Duncan@Univ Glasgow, Dept Stat, Glasgow G12 8QQ, Lanark, Scotland
Estimating and testing haplotype-trait associations in non-diploid populations	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00673.x	2009	26	Malaria is an infectious disease that is caused by a group of parasites of the genus Plasmodium. Characterizing the association between polymorphisms in the parasite genome and measured traits in an infected human host may provide insight into disease aetiology and ultimately inform new strategies for improved treatment and prevention. This, however, presents an analytic challenge since individuals are often multiply infected with a variable and unknown number of genetically diverse parasitic strains. In addition, data on the alignment of nucleotides on a single chromosome, which is commonly referred to as haplotypic phase, is not generally observed. An expectation-maximization algorithm for estimating and testing associations between haplotypes and quantitative traits has been described for diploid (human) populations. We extend this method to account for both the uncertainty in haplotypic phase and the variable and unknown number of infections in the malaria setting. Further extensions are described for the human immunodeficiency virus quasi-species setting. A simulation study is presented to characterize performance of the method. Application of this approach to data arising from a cross-sectional study of n=126 multiply infected children in Uganda reveals some interesting associations requiring further investigation.	Expectation-maximization algorithm,Genotype,Haplotype,Human immunodeficiency virus,Linear model,Malaria,Phenotype,Quasi-species,Strain	Li, X.); Thomas, BN (Thomas, B. N.@Rochester Inst Technol, Henrietta, NY USA::Rich, S. M.); Ecker, D (Ecker, D.); Tumwine, JK (Tumwine, J. K.@Makerere Univ, Kampala, Uganda::Foulkes, A. S.@Univ Massachusetts, Sch Publ Hlth & Hlth Sci, Amherst, MA 01003 USA
A Bayesian hierarchical distributed lag model for estimating the time course of risk of hospitalization associated with particulate matter air pollution	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2008.00640.x	2009	44	Time series studies have provided strong evidence of an association between increased levels of ambient air pollution and increased hospitalizations, typically at a single lag of 0, 1 or 2 days after an air pollution episode. Two important scientific objectives are to understand better how the risk of hospitalization that is associated with a given day's air pollution increase is distributed over multiple days in the future and to estimate the cumulative short-term health effect of an air pollution episode over the same multiday period. We propose a Bayesian hierarchical distributed lag model that integrates information from national health and air pollution databases with prior beliefs of the time course of risk of hospitalization after an air pollution episode. This model is applied to air pollution and health data on 6.3 million enrollees of the US Medicare system living in 94 counties covering the years 1999-2002. We obtain estimates of the distributed lag functions relating fine particulate matter pollution to hospitalizations for both ischaemic heart disease and acute exacerbation of chronic obstructive pulmonary disease, and we use our model to explore regional variation in the health risks across the USA.	Air pollution,Cardiovascular disease,Distributed lag model,Environmental epidemiology,Respiratory disease,Time series	Peng, Roger D.@Johns Hopkins Bloomberg Sch Publ Hlth, Dept Biostat, Baltimore, MD 21205 USA::Dominici, Francesca@Johns Hopkins Bloomberg Sch Publ Hlth, Dept Biostat, Baltimore, MD 21205 USA::Welty, Leah J.@Northwestern Univ, Feinberg Sch Med, Chicago, IL 60611 USA
Hierarchical Bayesian Markov switching models with application to predicting spawning success of shovelnose sturgeon	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2008.00642.x	2009	37	The timing of spawning in fish is tightly linked to environmental factors; however, these factors are not very well understood for many species. Specifically, little information is available to guide recruitment efforts for endangered species such as the sturgeon. Therefore, we propose a Bayesian hierarchical model for predicting the success of spawning of the shovelnose sturgeon which uses both biological and behavioural (longitudinal) data. In particular, we use data that were produced from a tracking study that was conducted in the Lower Missouri River. The data that were produced from this study consist of biological variables associated with readiness to spawn along with longitudinal behavioural data collected by using telemetry and archival data storage tags. These high frequency data are complex both biologically and in the underlying behavioural process. To accommodate such complexity we developed a hierarchical linear regression model that uses an eigenvalue predictor, derived from the transition probability matrix of a two-state Markov switching model with generalized auto-regressive conditional heteroscedastic dynamics. Finally, to minimize the computational burden that is associated with estimation of this model, a parallel computing approach is proposed.	Eigenvalue,Generalized auto-regressive conditional heteroscedasticity,Hierarchical Bayes model,Markov switching,Missouri River,Parallel computing,Stochastic volatility,Sturgeon,Telemetry	Holan, Scott H.@Univ Missouri, Dept Stat, Columbia, MO 65211 USA::Davis, Ginger M.@Univ Virginia, Charlottesville, VA USA
Joint analysis of correlated repeated measures and recurrent events processes in the presence of death, with application to a study on acquired immune deficiency syndrome	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2008.00641.x	2009	49	In many longitudinal studies, we observe two correlated processes: a repeated measures process and a recurrent events process, both subject to a dependent terminal event. For example, in the 'Terry Beirn community programs for clinical research on AIDS' (CPCRA) study, higher CD4 cell counts are associated with lower risk of recurrent opportunistic diseases. They are also correlated with mortality, e.g. higher CD4 cell repeated measures and a lower rate of opportunistic disease imply better survival for patients infected with the human immunodeficiency virus. We propose a joint random-effects model for the three correlated outcomes. The correlation is modelled by conditioning on shared random effects. Covariate effects can be taken into account in the model. Maximum likelihood estimation and inference are carried out through a Gaussian quadrature technique, assuming piecewise constant baseline hazards for recurrent events and death. The model can be fitted conveniently by Gaussian quadrature tools, e.g. SAS procedure NLMIXED. Simulation studies show that the estimation method yields satisfactory results. We apply this method to the CPCRA data.	Frailty model,Informative censoring,Longitudinal data analysis,Mixed effects model,Proportional hazards model	Liu, Lei@Univ Virginia, Dept Publ Hlth Sci, Div Biostat & Epidemiol, Charlottesville, VA 22908 USA::Huang, Xuelin@Univ Texas MD Anderson Canc Ctr, Houston, TX 77030 USA
Debiased estimation of proportions in group testing	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2008.00639.x	2009	27	In the assessment of disease, estimation of the proportion of infected units in a population can sometimes be facilitated by pooling units into groups for testing. Such group testing was used in a study of virus infection levels in carnation plants grown in glasshouses. In group testing problems, the maximum likelihood estimator is a biased estimator of the population proportion. We investigate the bias of the maximum likelihood estimator when testing groups of different size, using fixed and sequential procedures. The possibility of obtaining all positive groups contributes substantially to the bias. Analytical methods are shown to correct the bias for fixed procedures satisfactorily. For sequential procedures, with their uneven bias patterns, we propose a numerical method of correction which produces an almost unbiased estimator.	Bias correction,Estimation of proportions,Group testing,Sequential procedure	Hepworth, Graham@Univ Melbourne, Dept Math & Stat, Melbourne, Vic 3010, Australia::Watson, Ray@Univ Melbourne, Dept Math & Stat, Melbourne, Vic 3010, Australia
A functional approach to diversity profiles	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00646.x	2009	38	Diversity plays a central role in ecological theory and its conservation and management are important issues for the wellbeing and stability of ecosystems. The aim of this work is to provide a reliable theoretical framework for performing statistical analysis on ecological diversity by means of the joint use of diversity profiles and functional data analysis. We point out that ecological diversity is a multivariate concept as it is a function of the relative abundances of species in a biological community. For this, several researchers have suggested using parametric families of indices of diversity for obtaining more information from the data. Patil and Taillie introduced the concept of intrinsic diversity ordering which can be determined by using the diversity profile. It may be noted that the diversity profile is a non-negative and convex curve which consists of a sequence of measurements as a function of a given parameter. Thus, diversity profiles can be explained through a process that is described in a functional setting. Recent developments in environmental studies have focused on the opportunity to evaluate community diversity changes over space and/or correlation of diversity with environmental characteristics. For this, we develop an innovative analysis of diversity based on a functional data approach. Whereas conventional statistical methods process data as a sequence of individual observations, functional data analysis is designed to process a collection of functions or curves. Moreover, unconstrained models may lead to negative and/or non-convex estimates for the diversity profiles. To overcome this problem, a transformation is proposed which can be constrained to be non-negative and convex. We focus on some applications showing how functional data analysis provides an alternative way of understanding biological diversity and its interaction with natural and/or human factors.	Biological population,Diversity index,Diversity profile,Functional data analysis,Intrinsic diversity ordering,Smoothing splines	Gattone, Stefano A.@Univ Roma Tor Vergata, Dipartimento Studi Econ Finanziari & Metodi Quant, I-00133 Rome, Italy::Di Battista, Tonio@Univ G dAnnunzio, Pescara, Italy
Spatial prediction of weed intensities from exact count data and image-based estimates	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00664.x	2009	32	Collecting weed exact counts in an agricultural field is easy but extremely time consuming. Image analysis algorithms for object extraction applied to pictures of agricultural fields may be used to estimate the weed content with a high resolution (about 1 m(2)), and pictures that are acquired at a large number of sites can be used to obtain maps of weed content over a whole field at a reasonably low cost. However, these image-based estimates are not perfect and acquiring exact weed counts also is highly useful both for assessing the accuracy of the image-based algorithms and for improving the estimates by use of the combined data. We propose and compare various models for image index and exact weed count and we use them to assess how such data should be combined to obtain reliable maps. The method is applied to a real data set from a 30-ha field. We show that using image estimates in addition to exact counts allows us to improve the accuracy of maps significantly. We also show that the relative performances of the methods depend on the size of the data set and on the specific methodology (full Bayes versus plug-in) that is implemented.	Approximate Cox process,Gaussian random field,Image analysis,Model-based geostatistics,Multivariate data,Poisson regression,Precision farming,Spatial prediction	Guillot, Gilles@Univ Oslo, Dept Biol, Ctr Ecol & Evolutionary Synth, N-0316 Oslo, Norway::Loren, Niklas@Swedish Inst Food & Biotechnol, Gothenburg, Sweden::Rudemo, Mats@Chalmers, S-41296 Gothenburg, Sweden
Evaluating a continuous biomarker for infection by using observed disease status with covariate effects on disease	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2009.00681.x	2009	14	A continuous valued serology test has been developed for the infective agent causing bovine digital dermatitis. Since infection cannot be observed directly, disease status as indicated by foot lesions is used as an imperfect proxy to evaluate the test. The desire to specify covariate effects on disease rather than infection status leads to some difficulties in developing a coherent probability model linking the variables of interest. We resolve these issues by making some plausible conditional independence assumptions, and we use the resulting model to investigate the ability of the serology test to discriminate between infected and uninfected animals.	Bayesian statistics,Diagnostic test,Markov chain Monte Carlo methods,Receiver operating characteristic analysis,Sensitivity,Specificity	Jones, Geoffrey@Massey Univ, Inst Fundamental Sci, Palmerston North, New Zealand::Johnson, Wesley O.@Univ Calif Irvine, Irvine, CA USA
An application of multinomial logistic regression to estimating performance of a multiple-screening test with incomplete verification	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2007.00602.x	2008	15	The paper describes a method of estimating the performance of a multiple-screening test where those who test negatively do not have their true disease status determined. The methodology is motivated by a data set on 49927 subjects who were given K=6 binary tests for bowel cancer. A complicating factor is that individuals may have polyps in the bowel, a condition that the screening test is not designed to detect but which may be worth diagnosing. The methodology is based on a multinomial logit model for Pr(S vertical bar R-6), the probability distribution of patient status S (healthy, polyps or diseased) conditional on the results R-6 from six binary tests. An advantage of the methodology described is that the modelling is data driven. In particular, we require no assumptions about correlation within subjects, the relative sensitivity of the K tests or the conditional independence of the tests. The model leads to simple estimates of the trade-off between different errors as the number of tests is varied, presented graphically by using receiver operating characteristic curves. Finally, the model allows us to estimate better protocols for assigning subjects to the disease group, as well as the gains in accuracy from these protocols.	log-linear models,partial verification,receiver operating characteristic curves	Lloyd, Chris J.@Univ Melbourne, Melbourne Business Sch, Melbourne, Vic 3053, Australia::Frommer, Donald J.@St Vincents Hosp, Darlinghurst, NSW, Australia
Combining information from related meta-analyses of genetic association studies	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2007.00603.x	2008	19	When synthesizing data from genetic association studies researchers frequently perform several related meta-analyses, perhaps on different polymorphisms of the same gene, or on different outcomes, or they might define subgroups of studies by factors such as ethnicity, gender or study design. Current practice is to perform a totally separate meta-analysis of each set of studies; however, when the meta-analyses investigate related questions, it is possible that the estimates in one meta-analysis could be improved by using information from another. The meta-analytic model for a genetic association study can be parameterized in terms of four meaningful parameters: the size of the genetic effect, the genetic model, the allele frequency in controls and the degree of departure from Hardy-Weinberg equilibrium in controls. Even when the size of the genetic effect differs across meta-analyses, it may be possible to assume that some of the other parameters are common. The models are applied to a meta-analysis of the same gene-disease relationship in three different ethnic groups.	allele frequency,Bayesian methods,genetic association studies,genetic model,Hardy-Weinberg equilibrium,meta-analysis	Thompson, J. R.@Univ Leicester, Dept Hlth Sci, Leicester LE1 7RH, Leics, England::Minelli, C.@Univ London Imperial Coll Sci Technol & Med, London, England::Abrams, K. R.@Univ Leicester, Dept Hlth Sci, Leicester LE1 7RH, Leics, England::Thakkinstian, A.@Mahidol Univ, Bangkok 10700, Thailand::Attia, J.@Univ Newcastle, Newcastle, NSW 2308, Australia
Modelling the spread in space and time of an airborne plant disease	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2007.00612.x	2008	47	A spatiotemporal model is developed to analyse epidemics of airborne plant diseases which are spread by spores. The observations consist of measurements of the severity of disease at different times, different locations in the horizontal plane and different heights in the vegetal cover. The model describes the joint distribution of the occurrence and the severity of the disease. The three-dimensional dispersal of spores is modelled by combining a horizontal and a vertical dispersal function. Maximum likelihood combined with a parametric bootstrap is suggested to estimate the model parameters and the uncertainty that is attached to them. The spatiotemporal model is used to analyse a yellow rust epidemic in a wheatfield. In the analysis we pay particular attention to the selection and the estimation of the dispersal functions.	botanical epidemiology,disease occurrence,disease severity,dispersal function,plant disease epidemic,spatiotemporal model,yellow rust of wheat	Soubeyrand, Samuel@INRA, UR546 Biostat & Proc Spatiaux, F-84914 Avignon, France::Held, Leonhard@Univ Zurich, CH-8006 Zurich, Switzerland::Hoehle, Michael@Univ Munich, Munich, Germany::Sache, Ivan@INRA, F-78026 Versailles, France
A Bayesian statistical model for end member analysis of sediment geochemistry, incorporating spatial dependences	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2007.00615.x	2008	26	An important problem in the management of water supplies is identifying the sources of sediment. The paper develops a Bayesian approach, utilizing an end member model, to estimate the proportion of various sources of sediments in samples taken from a dam. This approach not only allows for the incorporation of prior knowledge about the geochemical compositions of the sources (or end members) but also allows for correlation between spatially contiguous samples and the prediction of the sediment's composition at unsampled locations. Sediments that were sampled from the North Pine Dam in south-east Queensland, Australia, are analysed to illustrate the approach.	compositional data,end member model,geochemistry,Markov chain Monte Carlo methods,sediments,spatial prediction	Palmer, Mark J.@Commonwealth Sci & Ind Res Org Math & Informat Sc, Wembley, WA 6913, Australia::Douglas, Grant B.@Commonwealth Sci & Ind Res Org Land & Water, Wembley, WA, Australia
Statistical methods to evaluate health effects associated with major sources of air pollution: a case-study of breathing patterns during exposure to concentrated Boston air particles	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2008.00618.x	2008	33	We conduct a case-study evaluating the source-specific effects of particulate matter on respiratory function. Using a structural equation approach, we assess the effect of different receptor models on the estimated source-specific effects for univariate respiratory response. Furthermore, we extend the structural equation model by placing a factor analysis model on the response to represent the measured respiratory responses in terms of underlying respiratory patterns. We estimate the particulate matter source-specific effects on respiratory rate, accentuated normal breathing and airway irritation and find a strong increase in airway irritation that is associated with exposure to motor vehicle particulate matter.	latent variables,particulate matter,receptor model,respiratory response,source apportionment,structural equation model	Nikolov, Margaret C.@Harvard Univ, Sch Publ Hlth, Boston, MA 02115 USA::Coull, Brent A.@Harvard Univ, Sch Publ Hlth, Boston, MA 02115 USA::Catalano, Paul J.@Harvard Univ, Sch Publ Hlth, Boston, MA 02115 USA::Diaz, Edgar@Harvard Univ, Sch Publ Hlth, Boston, MA 02115 USA::Godleski, John J.@Harvard Univ, Sch Publ Hlth, Boston, MA 02115 USA
Analysis of heat wave effects on health by using generalized additive model and bootstrap-based model selection	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2008.00626.x	2008	18	It is known that high summer temperature may lead to worsening health conditions among fragile individuals within exposed populations. It is also argued that multiday patterns of high temperature-heat waves-may have relevant effects on health. We discuss the possible measures of intensities of heat waves to be included in a generalized additive model explaining the number of hospital admissions that occurred during summer months in Milan. The issue of variable selection is central to the analysis: a computational method is discussed which may help in assessing the robustness of the model selection method. Eventually, we obtain evidence supporting the relevance of heat waves in driving adverse health episodes.	hospital admissions,Milan,non-linear temperature effect,unbiased risk estimator	Pauli, Francesco@Univ Padua, Dipartimento Sci Stat, I-35121 Padua, Italy::Rizzi, Laura@Univ Udine, I-33100 Udine, Italy
Semiparametric two-sample changepoint model with application to human immunodeficiency virus studies	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2008.00632.x	2008	29	A two-sample changepoint model is proposed to investigate the difference between two treatments or devices. Under our semiparametric approach, no assumptions are made about the underlying distributions of the measurements from the two treatments or devices, but a parametric link is assumed between the two. The parametric link contains the possible changepoint where the two distributions start to differ. We apply the maximum empirical likelihood for model estimation and show the consistency of the changepoint estimator. An extended changepoint model is studied to handle data censored because of detection limits in viral load assays of human immunodeficiency virus (HIV). Permutation and bootstrap procedures are proposed to test the existence of a changepoint and the goodness of fit of the model. The performance of the semiparametric changepoint model is compared with that of parametric models in a simulation study. We provide two applications in HIV studies: one is a randomized placebo-controlled study to evaluate the effects of a recombinant glycoprotein 120 vaccine on HIV viral load; the other is a study to compare two types of tubes in handling plasma samples for viral load determination.	changepoint,empirical distribution,likelihood ratio,maximum empirical likelihood estimation,profile likelihood	Hu, Zonghui@NIAID, Bethesda, MD 20892 USA::Qin, Jing@NIAID, Bethesda, MD 20892 USA::Follmann, Dean@NIAID, Bethesda, MD 20892 USA
Modelling species diversity through species level hierarchical modelling	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2005.00466.x	JAN 2005	35	Understanding spatial patterns of species diversity and the distributions of individ-ual species is a consuming problem in biogeography and conservation. The Cape floristic region of South Africa is a global hot spot of diversity and endemism, and the Protea atlas project, with about 60 000 site records across the region, provides an extraordinarily rich data set to model patterns of biodiversity. Model development is focused spatially at the scale of 1(') grid cells (about 37 000 cells total for the region). We report on results for 23 species of a flowering plant family known as Proteaceae (of about 330 in the Cape floristic region) for a defined subregion. Using a Bayesian framework, we developed a two-stage, spatially explicit, hierarchical logistic regression. Stage 1 models the potential probability of presence or absence for each species at each cell, given species attributes, grid cell (site level) environmental data with species level coefficients, and a spatial random effect. The second level of the hierarchy models the probability of observing each species in each cell given that it is present. Because the atlas data are not evenly distributed across the landscape, grid cells contain variable numbers of sampling localities. Thus this model takes the sampling intensity at each site into account by assuming that the total number of times that a particular species was observed within a site follows a binomial distribution. After assigning prior distributions to all quantities in the model, samples from the posterior distribution were obtained via Markov chain Monte Carlo methods. Results are mapped as the model-estimated probability of presence for each species across the domain. This provides an alternative to customary empirical 'range-of-occupancy' displays. Summing yields the predicted richness of species over the region. Summaries of the posterior for each environmental coefficient show which variables are most important in explaining the presence of species. Our initial results describe biogeographical patterns over the modelled region remarkably well. In particular, species local population size and mode of dispersal contribute significantly to predicting patterns, along with annual precipitation, the coefficient of variation in rainfall and elevation.	adaptive rejection method,Markov random field,spatial logistic regression,species range,species richness	
Assessing accuracy of a continuous screening test in the presence of verification bias	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/j.1467-9876.2005.00477.x	JAN 2005	26	In studies to assess the accuracy of a screening test, often definitive disease assessment is too invasive or expensive to be ascertained on all the study subjects. Although it may be more ethical or cost effective to ascertain the true disease status with a higher rate in study subjects where the screening test or additional information is suggestive of disease, estimates of accuracy can be biased in a study with such a design. This bias is known as verification bias. Verification bias correction methods that accommodate screening tests with binary or ordinal responses have been developed; however, no verification bias correction methods exist for tests with continuous results. We propose and compare imputation and reweighting bias-corrected estimators of true and false positive rates, receiver operating characteristic curves and area under the receiver operating characteristic curve for continuous tests. Distribution theory and simulation studies are used to compare the proposed estimators with respect to bias, relative efficiency and robustness to model misspecification. The bias correction estimators proposed are applied to data from a study of screening tests for neonatal hearing loss.	area under the curve,imputation,inverse probability weighting,mean score,neonatal hearing screening,receiver operating characteristic curve,semiparametric estimators,sensitivity,specificity	
A spatiotemporal model for Mexico City ozone levels	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1046/j.1467-9876.2003.05100.x	2004	22	We consider hourly readings of concentrations of ozone over Mexico City and propose a model for spatial as well as temporal interpolation and prediction. The model is based on a time-varying regression of the observed readings on air temperature. Such a regression requires interpolated values of temperature at locations and times where readings are not available. These are obtained from a time-varying spatiotemporal model that is coupled to the model for the ozone readings. Two location-dependent harmonic components are added to account for the main periodicities that ozone presents during a given day and that are not explained through the covariate. The model incorporates spatial covariance structure for the observations and the parameters that define the harmonic components. Using the dynamic linear model framework, we show how to compute smoothed means and predictive values for ozone. We illustrate the methodology on data from September 1997.	Bayesian inference,exponential variogram,Kriging,Markov chain Monte Carlo methods,spatiotemporal modelling,state space models,tropospheric ozone	
Bayesian texture segmentation of weed and crop images using reversible jump Markov chain Monte Carlo methods	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C	10.1111/1467-9876.00387	2003	26	A Bayesian method for segmenting weed and crop textures is described and implemented. The work forms part of a project to identify weeds and crops in images so that selective crop spraying can be carried out. An image is subdivided into blocks and each block is modelled as a single texture. The number of different textures in the image is assumed unknown. A hierarchical Bayesian procedure is used where the texture labels have a Potts model (colour Ising Markov random field) prior and the pixels within a block are distributed according to a Gaussian Markov random field, with the parameters dependent on the type of texture. We simulate from the posterior distribution by using a reversible jump Metropolis-Hastings algorithm, where the number of different texture components is allowed to vary. The methodology is applied to a simulated image and then we carry out texture segmentation on the weed and crop images that motivated the work.	classification,Gaussian Markov random field,image analysis,Ising model,Markov chain Monte Carlo methods,Metropolis-Hastings algorithm,mixture models,Potts model	